{"items":[{"id":"https://doi.org/10.59350/s4k4j-88p90","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0006-5800-1198","type":"Person","contributorRoles":["Author"],"givenName":"Aditya","familyName":"Arora"}],"date":{"published":"2025-04-29T08:40:00","updated":"2025-06-13T16:15:23"},"descriptions":[{"description":"A quick tutorial on creating an AI Calendar Assistant","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/s4k4j-88p90.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/s4k4j-88p90.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/s4k4j-88p90.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/s4k4j-88p90.xml"}],"identifiers":[{"identifier":"81185cee-5587-43ea-8681-086eb99ed47d","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/s4k4j-88p90","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://docs.n8n.io/hosting/installation/docker/","unstructured":"Docs.n8n.io. (2025). Docker | n8n Docs. https://docs.n8n.io/hosting/installation/docker/ ."},{"id":"https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/","unstructured":"Docs.n8n.io. (2025b). Google OAuth2 single service | n8n Docs. https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/#create-a-google-cloud-console-project"}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"How to install n8n on Mac and create an LLM pipeline using GPT API"}],"url":"https://hub.researchgraph.org/how-to-install-n8n-on-mac-and-create-an-llm-pipeline-using-gpt-api/","content":"<h1 id=\"introduction\">Introduction</h1><p>In this comprehensive guide, we will walk through creating an intelligent AI calendar assistant which will combine n8n’s workflow automation with OpenAI’s powerful GPT models. The best part? We’ll be running everything locally on your Mac, with full control over your data and no monthly subscription fees.</p><p>n8n stands for “nodemation” and is a powerful open-source workflow automation tool that connects various apps and services through an intuitive visual interface. Think of it as building with digital Lego blocks — each block (or node) represents a specific action or service that you can connect to create complex automations. What makes n8n particularly valuable is its ability to run locally while still offering enterprise-grade capabilities.</p><p>By integrating OpenAI’s GPT into our n8n workflow, we will create a natural language pipeline that transforms simple requests like “Schedule a meeting with Mark next Tuesday afternoon” into actual calendar events. Your AI assistant will check your availability, suggest suitable times, and handle the calendar mechanics for you — all through conversational interactions.</p><p>Although this blog focuses on Mac, the steps should be similar for Windows. Also, before we proceed, ensure you have the following:</p><ul><li><strong>Docker Installed:</strong>&nbsp;Docker Desktop on Mac/Windows or Docker Engine on Linux.</li><li><strong>API Credentials:</strong></li><li>An OpenAI API key (for GPT).</li><li>Google Cloud API credentials to integrate Calendar functionalities.</li></ul><h1 id=\"part-1-installing-n8n-locally-with-docker\">Part 1: Installing n8n Locally with Docker</h1><p>There are benefits of using Docker to install n8n:</p><ul><li>Simplified setup: Docker makes it easy to install and run n8n and its necessary dependencies.</li><li>Portability: Docker containers can easily move between different operating systems, avoiding compatibility issues and making it easy to migrate to various hosts or deploy to servers.</li></ul><p>From your terminal, run the following code:</p><pre><code>docker volume create n8n_data\ndocker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n</code></pre><p>This command creates a volume to store persistent data, downloads the required n8n image, and starts your container, exposed on port 5678. To save your work between container restarts, it also mounts a docker volume, n8n_data, to persist your data locally.</p><p>Once running, you can access n8n by opening:&nbsp;<a href=\"http://localhost:5678/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">http://localhost:5678</a> [1]</p><p>For every subsequent run, just type the second command in the terminal.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*-QiJPXQqGqvksBXe.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"435\"><figcaption><span style=\"white-space: pre-wrap;\">Commands running on terminal to run n8n locally</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*G3TbnGR8DO45uO2n.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"437\"><figcaption><span style=\"white-space: pre-wrap;\">n8n dashboard</span></figcaption></figure><h1 id=\"understanding-the-n8n-interface\">Understanding the n8n Interface</h1><p>Before diving into building our AI agent, let’s get familiar with the n8n interface:</p><ul><li><strong>Canvas</strong>: The central area where you’ll build your workflows</li><li><strong>Nodes</strong>: Building blocks of any workflow (triggers, actions, etc.)</li><li><strong>Editor/Executions tabs</strong>: Toggle between building and monitoring your workflow</li><li><strong>Save button</strong>: Located at the top to save your progress</li><li><strong>Sticky notes</strong>: Optional notes to keep your workflow organised</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/image-32.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"639\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/image-32.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/image-32.png 1000w, https://hub.researchgraph.org/content/images/2025/05/image-32.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">n8n Interface</span></figcaption></figure><h1 id=\"part-2-creating-an-llm-pipeline-using-the-gpt-api\">Part 2: Creating an LLM Pipeline Using the GPT API</h1><h2 id=\"building-a-basic-ai-agent\">Building a Basic AI Agent</h2><p>Now let’s create an AI agent that can understand natural language requests and respond intelligently using GPT:</p><p><strong>Step 1: Set Up the Trigger</strong></p><ol><li>Click the plus button in the middle of the canvas</li><li>In the search bar, type “chat” and select “Chat Trigger” from the results</li><li>This creates a trigger node that will act as the starting point for your workflow</li><li>You can rename this node by clicking the three dots and selecting “Rename”</li><li>The Chat Message trigger creates a native chat interface within n8n where we can test our agent</li></ol><p><strong>Step 2: Configure the AI Agent</strong></p><ol><li>Click the plus button after the Chat Message node to add a new node</li><li>Navigate to the “Advanced AI” section (or search for “AI Agent”)</li><li>Select “AI Agent” from the options</li><li>You’ll notice that the node expands to show three sections:</li></ol><ul><li>Chat Model (the LLM that powers your agent)</li><li>Memory (how your agent remembers previous interactions)</li><li>Tools (what capabilities your agent will have)</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/image-37.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"636\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/image-37.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/image-37.png 1000w, https://hub.researchgraph.org/content/images/2025/05/image-37.png 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Workflow with chat trigger and AI Agent</span></figcaption></figure><p><strong>Step 3: Set Up the Chat Model</strong></p><ol><li>Click on the “Chat Model” option</li><li>Select “OpenAI” from the dropdown menu</li><li>You now need to connect your OpenAI account:</li></ol><ul><li>Click “Select Credentials” then “Create New Credential”</li><li>Navigate to&nbsp;<a href=\"https://platform.openai.com/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">platform.openai.com</a></li><li>Sign up or log in to your account</li><li>Go to Settings →&nbsp;<a href=\"https://platform.openai.com/settings/organization/billing/overview?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">Billing</a>&nbsp;to set up payment (add $5–10 for testing)</li><li>Navigate to Settings →&nbsp;<a href=\"https://platform.openai.com/settings/organization/api-keys?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">API Keys</a></li><li>Click “Create new secret key”</li><li>Name it (e.g., “n8n Agent”) and select your project</li><li>Copy the generated API key (you won’t be able to see it again)</li></ul><p>4. Paste your API key into n8n’s credential form</p><p>5. Name your credential (e.g., “n8n Agent”) and click Save</p><p>6. Back in the Chat Model section, select your preferred model:</p><ul><li>For testing on a budget, choose “gpt-4o-mini”</li><li>For more complex tasks, consider “gpt-4.5 Preview” (more expensive but more capable)</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*6chJKq66bOfwNi9k\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">open ai platform to create new api key</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*Mv617mpx2eeQPf2b\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">n8n node to setup gpt credentials</span></figcaption></figure><p><strong>Step 4: Add Memory Capability</strong></p><ol><li>Click on the “Memory” option in your AI Agent node</li><li>Select “Window Buffer Memory” — this is the simplest memory type to get started with</li><li>Keep the default “Context Window Length” of 5 (this controls how many past messages the agent remembers)</li><li>The “Session ID Source” should automatically be set to pull from the Chat Message node</li><li>This memory configuration will allow your agent to maintain context throughout a conversation</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*n8IyVNyYmseQxtj4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Basic memory configuration for AI Agent</span></figcaption></figure><p><strong>Step 5: Add a Basic Tool for Testing</strong></p><ol><li>Click the plus button in the Tools section</li><li>For a simple test, search for and select “Calculator”</li><li>No configuration is needed for this basic tool</li><li>Save your workflow by clicking the Save button at the top of the screen</li><li>Click “Chat” to test your agent with some basic questions:</li></ol><ul><li>Try a question that doesn’t need calculation, like “What’s your favourite city?”</li><li>Then try a calculation question like “What is 2+5?”</li></ul><p>6. You can click on the execution flow to see exactly how the agent processed your request</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*o-ogWO-PTfiQk3UD\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Basic workflow with calculation capability</span></figcaption></figure><h1 id=\"creating-a-calendar-assistant\">Creating a Calendar Assistant</h1><p>Now let’s transform our basic agent into a powerful calendar assistant by connecting it to Google Calendar:</p><p><strong>Step 1: Configure Google Cloud for API Access</strong></p><ol><li>Remove the Calculator tool from your agent</li><li>Following the&nbsp;<a href=\"https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">documentation</a>, go to&nbsp;<a href=\"https://console.cloud.google.com/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">Google Cloud Console</a></li><li>Create a new project:</li></ol><ul><li>Click on the project dropdown at the top of the page</li><li>Select “New Project”</li><li>Name it “N8N Demo” (or something memorable)</li><li>Click “Create”</li></ul><p>4. Make sure your new project is selected in the dropdown</p><p>5. Enable the Google Calendar API:</p><ul><li>In the left sidebar, navigate to “APIs &amp; Services” → “Library”. Enable APIs and Services</li><li>Search for “Google Calendar API” and select it</li><li>Click “Enable”</li></ul><p>6. Set up the consent screen:</p><ul><li>Fill in required app information (name, support email, developer contact)</li><li>Skip optional fields and continue through the setup</li><li>For OAuth Client ID Application type, select “Web Application” and give it a name</li><li>Paste “<a href=\"http://localhost:5678/rest/oauth2-credential/callback?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">http://localhost:5678/rest/oauth2-credential/callback</a>” in the “Authorised Redirect URLs” section</li></ul><p>7. Click the “Edit OAuth Client” button and copy the “client secret key” and “client id”</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*O568DP0EEW6X9TP4\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"417\"><figcaption><span style=\"white-space: pre-wrap;\">google cloud console setup</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*ya1ZCWrI_-8ZOViy\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Client id and secret credentials</span></figcaption></figure><p><strong>Step 2: Add Calendar Event Creation Tool</strong></p><ol><li>Click the plus button in the Tools section of your AI Agent</li><li>Search for and select “Google Calendar Tool”</li><li>Click “Select Credentials” → “Create New Credential”</li><li>Paste in your Client ID and Client Secret from Google Cloud that we copied earlier</li><li>Click “Sign in with Google” and follow the authorisation flow</li><li>Once connected, configure the tool:</li></ol><ul><li>Set “Resource” to “Event”</li><li>Set “Operation” to “Create”</li><li>Select your calendar from the dropdown</li><li>For “Start Date” click “Expression” and enter: {{ $fromAI(‘start_date’) }}</li><li>For “End Date” click “Expression” and enter: {{ $fromAI(‘end_date’) }}</li><li>Under Additional Fields, click “Add Field” to add “Summary”</li><li>Select “Expression” and enter: {{ $fromAI(‘summary’) }}</li><li>Click “Add Field” to add “Description”</li><li>Select “Expression” and enter: {{ $fromAI(‘description’) }}</li></ul><p>7. For “Tool Description,” you can click “Set Manually” and enter: “This tool is used to create calendar events”</p><p>8. Rename the node to “Create Event” for clarity</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*rMoCeqX_3XZBSwXk\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Create event node configuration</span></figcaption></figure><p><strong>Step 3: Add Calendar Availability Search Tool</strong></p><ol><li>Add another Google Calendar tool to your agent</li><li>This tool will use the same Google credentials you already set up</li><li>Configure it as follows:</li></ol><ul><li>Set “Resource” to “Calendar”</li><li>Set “Operation” to “Availability”</li><li>Select your calendar from the dropdown</li><li>For “Start Time” click “Expression” and enter: {{ $fromAI(‘start_time’) }}</li><li>For “End Time” click “Expression” and enter: {{ $fromAI(‘end_time’) }}</li><li>Set “Output Format” to “Raw” (this gives the most complete data)</li><li>Set “Time Zone” to your local time zone (e.g., “Australia/Canberra”)</li></ul><p>4. For “Tool Description,” enter: “This tool is for searching for available time slots in my calendar”</p><p>5. Rename the node to “Search Availability”</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*tB1ijYrCP94EgIhl\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Search availability node configuration</span></figcaption></figure><h1 id=\"troubleshoot\">Troubleshoot:</h1><p>While trying to sign in with google above, you may encounter the following error:</p><p>Client authentication failed (e.g., unknown client, no client authentication included, or unsupported authentication method). More details {\"error\":\"invalid_client\",\"error_description\":\"Unauthorized\"} Failed to connect. The window n8n can be closed now.</p><p>To solve this issue, go to the “Audience” page and add your email address under “Test users”.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*mWeivHQpkYOWamit\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Adding test user</span></figcaption></figure><p><strong>Step 4: Create a Detailed System Message</strong></p><ol><li>Click on the main AI Agent node</li><li>Click “Add Option” and select “System Message”</li><li>This is where you give your agent its personality and instructions</li><li>Enter a detailed system message:</li></ol><p>Your role is my calendar assistant. Your primary responsibilities are:<br>1. Helping me find available time slots in my calendar<br>2. Creating new calendar events when requested<br>3. Managing my schedule efficiently<br>When I ask about availability, use the Search Availability tool to check my calendar first. When I ask to schedule something, first check if the time is available before creating an event.<br>Always confirm the details with me before creating a new event, including:<br>- The exact date and time<br>- Duration of the event<br>- Summary (title) of the event<br>- Any additional description needed<br>Be conversational and helpful in your responses.<br>Today is {{ $now.format('cccc') }} the {{ $now.format('yyyy-MM-dd HH:mm') }}.<br>Be conversational and helpful in your responses.<br>Note: The date expression ensures the agent always knows the current date and time, which is crucial for calendar operations</p><p><strong>Step 5: Save and Test Thoroughly</strong></p><ol><li>Save your workflow by clicking the Save button</li><li>Click “Chat” to open the testing interface</li><li>Test availability searches with queries like:</li></ol><ul><li>“What’s my availability for tomorrow?”</li><li>“Do I have any free time next Tuesday afternoon?”</li><li>“Show me my available slots for the rest of the week”</li></ul><p>4. Test event creation with queries like:</p><ul><li>“Schedule a meeting with John tomorrow at 2 PM”</li><li>“Create a dentist appointment next Wednesday at 10 AM”</li><li>“Add a team stand-up every Monday at 9 AM for the next 4 weeks”</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*s1_VwYa4J42G7KbB\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Testing our workflow</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*ufb4SCH8zavco4ps\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Testing our workflow</span></figcaption></figure><p>Common Issues and Solutions:</p><ul><li><strong>Wrong Date/Time</strong>: Make sure your system message includes the current date expression</li><li><strong>No Available Slots</strong>: Check the output format in the Search Availability node</li><li><strong>API Errors</strong>: Verify your Google Calendar permissions are set correctly</li></ul><h1 id=\"conclusion-and-next-steps\">Conclusion and Next Steps</h1><p>Congratulations! You now have a working AI calendar assistant. Here are some ways to extend it:</p><ul><li>Add more tools for rescheduling or canceling events</li><li>Improve the system message with more specific instructions</li><li>Connect it to other communication channels like email or Slack</li></ul><p>By combining n8n’s workflow automation with OpenAI’s GPT models, you’ve created a powerful AI assistant that saves you time on calendar management. The possibilities for extending this approach to other areas of your work are endless!</p><h1 id=\"references\">References</h1><ul><li>Docs.n8n.io. (2025). <em>Docker | n8n Docs</em>. <a href=\"https://docs.n8n.io/hosting/installation/docker/?ref=hub.researchgraph.org\">https://docs.n8n.io/hosting/installation/docker/</a> .</li><li>Docs.n8n.io. (2025b). <em>Google OAuth2 single service | n8n Docs</em>. <a href=\"https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/?ref=hub.researchgraph.org#create-a-google-cloud-console-project\">https://docs.n8n.io/integrations/builtin/credentials/google/oauth-single-service/#create-a-google-cloud-console-project</a></li></ul><div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/how-to-install-n8n-on-mac-and-create-an-llm-pipeline-using-gpt-api-2e2ec367aed0?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/ChatGPT-Image-May-15--2025--12_59_10-PM.png","state":"findable"},{"id":"https://doi.org/10.59350/fjhdq-ga249","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0006-5800-1198","type":"Person","contributorRoles":["Author"],"givenName":"Aditya","familyName":"Arora"}],"date":{"published":"2025-04-29T02:00:00","updated":"2025-06-13T16:14:43"},"descriptions":[{"description":"A Beginner’s Guide","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/fjhdq-ga249.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/fjhdq-ga249.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/fjhdq-ga249.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/fjhdq-ga249.xml"}],"identifiers":[{"identifier":"5be53d09-b78e-4f25-88e1-7220c4f67419","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/fjhdq-ga249","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://www.youtube.com/watch?v=ocMOZpuAMw4","unstructured":"Cursor Tutorial for Beginners (AI Code Editor).  https://www.youtube.com/watch?v=ocMOZpuAMw4"}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"Developing a REST API in Python and Flask Using Cursor Editor and AI"}],"url":"https://hub.researchgraph.org/developing-a-rest-api-in-python-and-flask-using-cursor-editor-and-ai/","content":"<h1 id=\"introduction\">Introduction</h1><p>REST APIs are essential for modern web applications, enabling seamless communication between frontend and backend services. Building them from scratch can be time-consuming, but with the right tools, we can streamline the process. This is where&nbsp;<strong>Cursor Editor</strong>&nbsp;comes in.</p><p>Cursor is a code editor similar to VS Code but with powerful AI capabilities built-in. It helps developers write code faster, understand complex concepts, and solve problems more efficiently. In this tutorial, we’ll leverage these AI features to build our API with minimal manual coding.</p><p>This post will guide you through building a functional REST API using Python, Flask, and the Cursor Editor with AI assistance. Our API will support basic operations for searching and reading from a CSV file task list. We’ll see how Cursor’s AI features can significantly speed up development by generating code, explaining concepts, debugging, and suggesting improvements.</p><h1 id=\"project-setup\">Project Setup</h1><h2 id=\"prerequisites\">Prerequisites</h2><p>Before we begin, make sure you have the following installed:</p><ol><li><a href=\"https://www.python.org/downloads/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">Python</a>&nbsp;(3.7 or higher)</li><li><a href=\"https://cursor.sh/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">Cursor Editor</a>&nbsp;(free version is sufficient for this tutorial)</li><li><a href=\"https://www.postman.com/downloads/?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">Postman</a>&nbsp;(for testing the API)</li></ol><h1 id=\"creating-a-new-project\">Creating a New Project</h1><ol><li>Open Cursor Editor</li><li>Go to File &gt; Open Folder</li><li>Create a new folder on your desktop or preferred location</li><li>Open this folder in Cursor</li></ol><h1 id=\"setting-up-virtual-environment\">Setting up virtual environment</h1><ol><li>On Mac or Linux:</li></ol><pre><code>python3 -m venv .venv\nsource .venv/bin/activate</code></pre><p>2. On Windows</p><pre><code>py -m venv .venv\nsource .venv/Scripts/activate</code></pre><p>A virtual environment (venv) is an isolated Python environment that helps you with:</p><ol><li><strong>Project Isolation</strong></li></ol><ul><li>Each project gets its own independent set of Python packages</li><li>Packages installed in one virtual environment don’t interfere with other projects</li><li>You can have different versions of the same package for different projects</li></ul><p><strong>2. Dependency Management</strong></p><ul><li>In your case, when you install packages using (pip install -r requirements.txt), Flask and python-dateutil will be installed only in your (.venv) environment</li><li>This prevents conflicts with system-wide Python packages</li><li>Makes it easier to track and manage project-specific dependencies</li></ul><p><strong>3. Clean Development</strong></p><ul><li>Prevents polluting your global Python installation</li><li>Makes it easier to recreate the development environment on another machine</li><li>Helps avoid the “it works on my machine” problem</li></ul><h1 id=\"understanding-cursor%E2%80%99s-ai-features\">Understanding Cursor’s AI Features</h1><p>Before we dive into coding, let’s familiarise ourselves with the key AI features of Cursor that we’ll be using:</p><ol><li>Composer (Ctrl+I / Cmd+I): Generates multiple files at once, perfect for scaffolding a new project.</li><li>Chat Window (Ctrl+L / Cmd+L): Context-aware AI chat that can explain code, suggest improvements, and answer questions.</li><li>Inline Editing (Ctrl+K): Make targeted code changes by highlighting code and instructing the AI what to modify.</li><li>Inline Completions: As you type, Cursor suggests completions that you can accept with Tab.</li></ol><p>AI works best at solving really small, discrete, detailed tasks. The more specific you can be, the more context and information you give it, the better results you’re going to get.</p><h1 id=\"designing-our-csv-task-structure\">Designing Our CSV Task Structure</h1><p>For our API, we’ll use a simple CSV file to store task data. Here’s the structure we’ll implement:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://miro.medium.com/v2/resize:fit:840/1*JQfeMEIzcPufKWOWoj1SnA.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"214\"></figure><h1 id=\"building-the-api-with-cursor%E2%80%99s-help\">Building the API with Cursor’s Help</h1><p>Step 1: Project Scaffolding with Composer</p><p>Let’s use Cursor’s Composer feature to generate our basic Flask application structure.</p><ol><li>Press Ctrl+I (or Cmd+I on Mac) to open the Composer</li><li>Expand the panel to see more and enter the following prompt:</li></ol><p>Create a Flask REST API application for managing tasks stored in a CSV file. The API should have:</p><p>1. A main app.py file with Flask setup and configuration</p><p>2. A tasks.py file for reading and querying the CSV task list</p><p>3. Routes for listing all tasks and searching tasks by keyword or status</p><p>4. Basic error handling</p><p>5. A sample tasks.csv file with at least 5 example tasks</p><p>Use the following structure for each task: id, title, description, status, due_date, priority</p><ol><li>Press Enter and wait for Cursor to generate the files</li><li>Review the generated files and click “Accept All” when you’re satisfied</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*xwfJsxLSqF7TjhL1\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Code Editor Composer view</span></figcaption></figure><p>Step 2: Examining and Understanding the Generated Code</p><p>Let’s use the Chat feature to understand what Cursor has created for us.</p><ol><li>Press Ctrl+L (or Cmd+L on Mac) to open the Chat window</li><li>Add context by typing app.py and selecting it from the suggestions</li><li>Ask: “Can you explain what this code does and how the API endpoints are structured?”</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*SqEvTu-YLVED5Tb8\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">Code explanation with context of file</span></figcaption></figure><p>Step 3: Enhancing the Task Search Functionality</p><p>Let’s use the inline editing feature to improve the search functionality.</p><ol><li>In the tasks.py file, locate the search function</li><li>Highlight the function</li><li>Press Ctrl+K</li><li>Enter the prompt: “Enhance this function to allow searching by priority level (high, medium, low) and due date range”</li><li>Review the suggested changes and click “Accept” if satisfied</li></ol><p>Step 4: Adding Input Validation</p><p>Now let’s add input validation to our API endpoints:</p><ol><li>Open the Chat window with Ctrl+L</li><li>Add context by typing app.py and selecting it</li><li>Ask: “Can you help me add input validation for the search endpoint to ensure valid parameters?”</li><li>Review the code Cursor suggests and click “Apply” to implement it</li></ol><h1 id=\"testing-the-api-with-postman\">Testing the API with Postman</h1><p>Now that our API is ready, let’s test it using Postman:</p><ol><li>Make sure your Flask server is running (with a command like python3 app.py)</li><li>Open Postman</li><li>Create a new request to GET&nbsp;<a href=\"http://localhost:5001/api/tasks?ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">http://localhost:5001/api/tasks</a>&nbsp;to list all tasks</li><li>Create another request to GET&nbsp;<a href=\"http://127.0.0.1:5001/api/tasks?keyword=bug&ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">http://127.0.0.1:5001/api/tasks?priority=</a>high to search for tasks with priority “high”</li><li>Try other search parameters like status=pending or keyword=bug</li></ol><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*u4Fq8AcphJWKhC8s\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"438\"><figcaption><span style=\"white-space: pre-wrap;\">API testing using Postman</span></figcaption></figure><p>If you encounter any issues, use Cursor’s Chat feature to help debug:</p><ol><li>Open Chat with Command / Ctrl+L</li><li>Paste any error messages you’re seeing</li><li>Ask: “What might be causing this error and how can I fix it?”</li></ol><h1 id=\"conclusion\">Conclusion</h1><p>In this tutorial, we’ve built a functional REST API using Python and Flask with minimal manual coding. Cursor Editor’s AI features helped us in several ways:</p><ol><li>Code Generation: We used Composer to scaffold our entire project</li><li>Understanding Code: The Chat feature explained the code structure</li><li>Enhancing Functionality: We improved our search feature with inline editing</li><li>Debugging: Any issues were quickly resolved with AI assistance</li></ol><p>This approach to development can significantly increase your productivity, especially when working with new frameworks or technologies. Cursor’s AI capabilities allow you to focus on the architecture and design while offloading repetitive coding tasks.</p><p>What’s next? You could extend this API by adding write operations (POST, PUT, DELETE), implementing authentication, or connecting to a database instead of a CSV file. And with Cursor’s AI features, these enhancements can be implemented quickly and efficiently.</p><h1 id=\"references\">References</h1><ul><li>Cursor Tutorial for Beginners (AI Code Editor). &nbsp;<a href=\"https://www.youtube.com/watch?v=ocMOZpuAMw4&ref=hub.researchgraph.org\" rel=\"noopener ugc nofollow\">https://www.youtube.com/watch?v=ocMOZpuAMw4</a></li></ul><div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/developing-a-rest-api-in-python-and-flask-using-cursor-editor-and-ai-661728d30b55?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/featuredimage.png","state":"findable"},{"id":"https://doi.org/10.59350/gg100-4qm12","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0007-4527-7926","type":"Person","contributorRoles":["Author"],"givenName":"Harshitha","familyName":"Thoram"}],"date":{"published":"2025-04-29T02:00:00","updated":"2025-06-13T16:14:20"},"descriptions":[{"description":"LLM-Graph-Builder: Converting Data into a Knowledge Graph","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/gg100-4qm12.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/gg100-4qm12.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/gg100-4qm12.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/gg100-4qm12.xml"}],"identifiers":[{"identifier":"653444a3-987b-4ed0-ac2e-6025eae39c98","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/gg100-4qm12","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://en.wikipedia.org/wiki/Apple_Inc.","unstructured":"projects, C. to W. (2001). <i>Apple Inc.</i>. Wikimedia Foundation, Inc. https://en.wikipedia.org/wiki/Apple_Inc."}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"LLM Graph Builder"}],"url":"https://hub.researchgraph.org/llm-graph-builder/","content":"\n<!--kg-card-begin: html-->\n\n<h2>Introduction</h2>\n<p>Creating structured graphs out of unstructured text can be both exciting and challenging. At its core, you’re transforming free-form sentences into connected entities and relationships—effectively turning text into a network of information that’s much easier to analyse and query.</p>\n<p>This concept of extracting knowledge from text and structuring it as graphs isn’t entirely new; however, the rise of Large Language Models (LLMs) has propelled it into the mainstream. As LLMs continue to mature, they provide increasingly sophisticated ways to capture entities, attributes, and relationships from raw text sources.</p>\n<h2>Why Build a Knowledge Graph from Text?</h2>\n<p>One primary motivation is to support Retrieval-Augmented Generation (RAG). Using text embedding models on unstructured data can sometimes work well, but falls short with complex, multi-hop queries or questions requiring filtering, sorting, or aggregations across multiple entities. When you convert text into a knowledge graph, you gain a structured framework that is:</p>\n<ul>\n<li>\n<p><strong>Highly Queryable</strong>: Graph databases allow for easy querying of relationships between entities, enabling precise retrieval.</p>\n</li>\n<li>\n<p><strong>Scalable</strong>: New information or data sources can be integrated without disrupting existing structures.</p>\n</li>\n<li>\n<p><strong>Adaptable to Complex Questions</strong>: With a graph, you can chain multiple relationships and properties to answer queries that would be difficult (or impossible) with unstructured text alone.</p>\n</li>\n</ul>\n<h2>The LLM Graph Transformer</h2>\n<p>Over the past year, developers and researchers—including contributors to LangChain—have experimented with using LLMs to build knowledge graphs. These experiments revealed insights into the best practices for converting large amounts of text into meaningful, linked entities. As a result, a component called the <strong>LLM Graph Transformer</strong> was added to LangChain, simplifying the process of extracting structured knowledge from text.</p>\n<h2>Building your Graph Using Neo4j (Web version)</h2>\n<p>Neo4j is a popular graph database known for its flexibility and robust tooling.</p>\n<ul>\n<li>\n<p>Set up a Free Neo4j Instance Aura(Hosted)</p>\n</li>\n<li>\n<p>Sign Up: Go to <a href=\"https://neo4j.com/product/auradb/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=APAC-Search-SEMBrand-Evergreen-None-SEM-SEM-NonABM&utm_term=aura%20db&utm_adgroup=auradb&gad_source=1&gclid=CjwKCAjwp8--BhBREiwAj7og1x0I1SyB1lGGw2l71btjTSqR7KyvpR6mO_iPK_ga6UGHi6C4TFJlYRoC-i0QAvD_BwE\">Neo4j Aura</a> and create an account (there’s a free tier available).</p>\n</li>\n<li>\n<p>From the Aura dashboard, click <strong>“New Instance”</strong> and select the <strong>Free</strong> tier.</p>\n</li>\n<li>\n<p>Give your database a name, choose a region, and wait for it to spin up.</p>\n</li>\n<li>\n<p>Once your database is live, click on “Connection” or “Details” to find the Bolt URL, username, and password.</p>\n</li>\n<li>\n<p>The URL often looks like <code>neo4j+s://&lt;your_db_id&gt;.databases.neo4j.io</code>.</p>\n</li>\n<li>\n<p>Keep these credentials safe for the next step.</p>\n</li>\n<li>\n<p>Configure llm-graph-builder for Neo4J</p>\n</li>\n</ul>\n<p>Open the <a href=\"https://llm-graph-builder.neo4jlabs.com/?ref=hub.researchgraph.org\">LLM-Knowledge Graph Builder</a></p>\n<p>Whether you installed Neo4j locally or you’re using a hosted solution like Neo4j Aura (free tier or paid), you’ll need to capture your connection details like uri, username and password for llm-graph-builder.</p>\n<p>Once an instance is connected, we can upload or link to text documents, articles, websites—whatever unstructured text you want as shown in the image</p>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/neo4j-scaled-1.png\"/></p>\n</figure>\n<p>Below is a simple demonstration of how you might take a Wikipedia article, load it into your LLM-Knowledge Graph Builder connected to Neo4j, and then run a quick question-answering session against the generated knowledge graph.. This will show you end-to-end how to <strong>ingest</strong> unstructured text, <strong>build</strong> a graph, and finally <strong>query</strong> that graph via the integrated chat interface.</p>\n<p>For this example, we’ll use the Wikipedia page for <strong>Apple Inc.</strong> (just as an illustrative sample).</p>\n<p>Navigate to web sources and choose wikipedia. Then enter the link <a href=\"https://en.wikipedia.org/wiki/Apple_Inc.?ref=hub.researchgraph.org\">Apple inc</a> to upload the wikipedia information on apple</p>\n<p>You’ll typically see the document’s “Name” and “Status” columns. Once uploaded, the Status may indicate “Uploaded” or “Ready.”</p>\n<p><strong>Generating the Graph</strong></p>\n<p>Once your text or data source is uploaded to the LLM-Knowledge Graph Builder, the process of creating the graph begins:</p>\n<p><strong>Extraction via LLM</strong>: The builder sends the text in batches to the selected LLM (e.g., “OpenAI GPT-4”), which identifies potential entities (people, organizations, products, dates, etc.) and the relationships between them (founded_by, created, headquartered_in, etc.). This step relies on a specialised prompt that guides the model to focus on relevant information.</p>\n<p><strong>Node and Edge Creation:</strong> As the model identifies entities and relationships, the tool automatically creates nodes (for entities) and edges (for relationships) in the connected Neo4j database. In many cases, a structured preview is provided, allowing verification before finalizing. The process generated <strong>621 nodes</strong> and <strong>1,728 relationships</strong> as shown, highlighting just how much information can be extracted from what might seem like a “basic” text source.</p>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/graph-scaled-1.png\"/></p>\n<p><strong>Note</strong>: If you’re using LangChain or a similar framework programmatically, you can specifically set the entities or relationships to look for. For instance, you might only want People, Companies, Founding Years, and Headquarters. However, in the current web version of the LLM-Knowledge Graph Builder, these extraction rules aren’t yet customizable—you simply rely on the model’s generic extraction capabilities.</p>\n<p>There is also LLM-Knowledge Graph Chat panel on the right as shown in the above image that allows you to pose natural-language queries about the ingested documents or videos. For instance:</p>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/chat-1.png\"/></p>\n<p>The tool arrives at this answer by traversing the knowledge graph for any nodes related to “Apple Inc.” via <code>FOUNDED_BY</code> (or a similarly labeled edge), collecting those person nodes, and then constructing a natural-language response for you.</p>\n<p>Instead of a single-hop question (like founders), you can chain relationships. For example, “Which products were introduced by Apple after 2007?” or “What roles did Steve Jobs hold at the company?”</p>\n<p>By following these steps, you can quickly turn public (or internal) text sources into an interactive knowledge graph.</p>\n<h2>Conclusion</h2>\n<p>As textual data (or even video transcripts, audio metadata, articles, etc.) grows in volume and complexity, building a knowledge graph allows for efficient querying, filtering, and analysis. Instead of searching through disorganized blocks of text, you have a structured, interconnected data model. This is particularly important for Retrieval-Augmented Generation (RAG) workflows, where LLMs benefit from being able to quickly reference precise, interconnected facts rather than scanning large, unstructured documents.</p>\n\n<h2>References</h2>\n<ul>\n<li>Wikipedia Contributors (2019). Apple Inc. Wikipedia. <a>https://en.wikipedia.org/wiki/Apple_Inc.</a>.</li>\n</ul>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/an-introduction-to-llm-graph-builder-f309920fd2c1?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/featuredimage-1.webp","state":"findable"},{"id":"https://doi.org/10.59350/4sjej-74363","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0009-3354-7794","type":"Person","contributorRoles":["Author"],"givenName":"Immanuel Alvaro","familyName":"Bhirawa"}],"date":{"published":"2025-04-29T02:00:00","updated":"2025-06-13T16:15:56"},"descriptions":[{"description":"Tutorial for Windows Users","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/4sjej-74363.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/4sjej-74363.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/4sjej-74363.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/4sjej-74363.xml"}],"identifiers":[{"identifier":"baec2d76-1fb1-436c-8a65-7948465b2169","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/4sjej-74363","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://www.docker.com/","unstructured":"\nhttps://www.docker.com\n"},{"id":"https://github.com/n8n-io/self-hosted-ai-starter-kit","unstructured":"\nhttps://github.com/n8n-io/self-hosted-ai-starter-kit\n"},{"id":"https://ollama.com/","unstructured":"\nhttps://ollama.com\n"},{"id":"https://ollama.com/library/qwen2.5:1.5b","unstructured":"\nhttps://ollama.com/library/qwen2.5:1.5b\n"}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"How to install n8n and create an LLM pipeline using Ollama &amp; Docker"}],"url":"https://hub.researchgraph.org/how-to-install-n8n-and-create-an-llm-pipeline-using-ollama-docker/","content":"\n<!--kg-card-begin: html-->\n<h2>Introduction</h2>\n<p>Large language models (LLMs) are transforming the way we engage with technology, delivering remarkable capabilities in natural language understanding and generation. However, incorporating these models into your projects can feel overwhelming due to challenges like managing dependencies and ensuring compatibility across systems. In this blog post, we offer a detailed guide to installing <strong>n8n</strong>, a versatile workflow automation tool, and building an LLM pipeline using <strong>Ollama</strong> and <strong>Docker</strong> on a Windows environment. By the end of this tutorial, you’ll have a fully operational setup that lets you run LLMs locally and automate tasks seamlessly through n8n’s intuitive interface. Whether you’re a seasoned developer or a curious beginner, this step-by-step journey will equip you with the tools and insights to tap into the power of LLMs for your workflows. Let’s get started!</p>\n<h2>Before we start, What exactly is n8n?</h2>\n<p>n8n is an open-source workflow automation tool designed to simplify the process of connecting apps, services, and APIs without extensive coding. Pronounced “nodemation,” it allows users to create complex workflows through an intuitive, visual interface where nodes represent actions—like triggering events, processing data, or integrating with tools like Ollama for LLMs. In this tutorial, we’re using n8n to orchestrate our LLM pipeline, enabling us to automate tasks such as feeding prompts to a language model and handling outputs efficiently. Its flexibility, extensive library of pre-built integrations, and ability to run locally make it an ideal choice for developers and hobbyists alike who want to harness the power of automation on a Windows setup with Docker.</p>\n<h2>What exactly is Ollama and why do we need it?</h2>\n<p>Ollama is an open-source tool that simplifies running and managing large language models (LLMs) locally on your machine. It provides an easy-to-use framework to download, configure, and interact with powerful LLMs, like LLaMA or other supported models, without relying on cloud services. In this tutorial, we’re using Ollama because it allows us to integrate LLMs into our n8n workflows seamlessly via Docker, giving us full control over the models and their outputs right from our Windows setup. This local approach not only boosts privacy and reduces latency but also eliminates dependency on external APIs, making it perfect for experimenting with or deploying LLMs in a cost-effective, self-contained pipeline.</p>\n<h2>Lastly, what exactly is Docker and why it is so good for self-hosting</h2>\n<p>Docker is an open-source platform that uses containerisation to package applications and their dependencies into lightweight, portable units called containers. These containers run consistently across different environments, making Docker a game-changer for self-hosting projects like our LLM pipeline. Why is it so good? Docker ensures that n8n, Ollama, and our LLMs operate in isolated, reproducible environments, avoiding conflicts between software versions or system settings. This portability means you can host everything on your Windows machine with confidence, knowing it’ll work the same elsewhere. Plus, its efficiency in resource use and ease of deployment make self-hosting simpler, more secure, and scalable—all key for running powerful tools locally without the headache.</p>\n<p>Now that we’ve covered the essentials—n8n for automation, Ollama for LLMs, and Docker for a rock-solid foundation—let’s dive into the hands-on part. Below, we’ll walk through each step to get this pipeline up and running on your Windows system, starting with setting up Docker. Ready? Here we go!</p>\n<h2>First Step: Installing Docker</h2>\n<p>To kick off our journey of building an LLM pipeline with n8n and Ollama, the first step is installing Docker—a powerful platform that lets us develop, ship, and run applications inside containers. But why are we using Docker for this? The answer lies in its ability to streamline and stabilise our setup. Docker packages all the necessary components—code, libraries, and configurations—into containers, ensuring that our pipeline works the same way on any machine, whether it’s your local Windows computer or a cloud server. This consistency eliminates frustrating compatibility issues, like the infamous “it works on my machine” problem. Beyond that, Docker keeps the different parts of our pipeline, such as n8n and Ollama, isolated from each other. This isolation means that if one component runs into trouble, it won’t derail the others, making it easier to manage and fix issues. Plus, Docker’s containerisation makes deployment a breeze and offers the flexibility to scale up or replicate our setup later on. In essence, Docker provides a portable, reliable foundation that sets us up for success with our LLM project.</p>\n<p>Now, let’s see how to install Docker in your wonder machine!</p>\n<ol>\n<li>Go to the Docker Website here! and download Docker Desktop as shown in the image below. After your download is done, don’t forget to run the .exe file</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-4-1024x462.png\"/></p>\n<figcaption>This image was created by the author and sourced from <a href=\"https://www.datacamp.com/tutorial/local-ai?ref=hub.researchgraph.org\">Docker</a></figcaption>\n</figure>\n<ol>\n<li>After the .exe is run, the result should look like this. Restart your PC!</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-1-1024x609.jpeg\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>After the docker is installed, you should open the docker app if it hasn’t already opened by itself, and just follow the recommended settings. Once that’s done, you should be in this page and you should connect your account as below, and that’s it for Docker installation!</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-5-1024x601.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<h2>Second step: Installing n8n</h2>\n<p>After installing docker, the next step is installing n8n and then Ollama. Because we installed Docker, the following steps should be straightforward!</p>\n<ol>\n<li>So the first step would be to go to your images and in your search bar you type n8nio/n8n as such. Make sure the tag is latest, and then what you have to do is click on pull. This will pull the image to your Docker.</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-6-1024x605.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>\n<p>After the pull process is done, go back to the image section within docker and run the n8nio/n8n image that you just pulled. At this stage, there are additional settings that you need to keep in mind. Aside from putting in the container name (this can be up to your choosing), you need to enter the Host port number and Volumes’ Host path as well as the Container Path as such. A few things to note,</p>\n</li>\n<li>\n<p>For the host port number, if you don’t know much about ports, put the number as <code>5678</code> as the number and Docker should work fine!</p>\n</li>\n<li>\n<p>Additionally, for the Host path for volumes, that basically acts as the storage folder for your n8n container to store files to, so you can use any folder that you like!</p>\n</li>\n<li>\n<p>Lastly, for the Container path, make sure to put specifically “/home/node/.n8n”. What this does is, it will ensure the data in your host machine is synced to n8n’s directory inside this Docker container. This will ensure all the data is saved in the correct area.</p>\n</li>\n<li>\n<p>Environment variables are optional for now, so we can skip them.</p>\n</li>\n</ol>\n<p>So, after all the settings have been set, you are free to run it!</p>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-7-1024x601.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>After everything is done, the Docker should look like this. All you have to do is copy the localhost URL to access your n8n localhost server, and you’re done! The next final step is to set up your n8n account, and you can use it promptly! Follow the steps that are listed when you are setting up n8n, such as your email address, full name, etc. And <strong>don’t forget</strong> to activate the activation key that you will get from your email address. Then you’re done with n8n!</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-8-1024x605.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<h2>Third Step: Installing Ollama</h2>\n<ol>\n<li>To install Ollama to self-host LLMs in your local machine in n8n, the first step that you need to take is to go to this github repository -&gt; <a href=\"https://github.com/n8n-io/self-hosted-ai-starter-kit?ref=hub.researchgraph.org\">n8n Self Hosted AI Starter Kit Github Respository</a> and scroll down until you find the “For Nvidia GPU users code block as shown below</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-9-1024x553.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>\n<p>You copy that code block and paste it to your Docker terminal as such, and press enter to run it. If your Windows machine does not use Nvidia as it’s GPU, you can do the following code blocks instead:</p>\n</li>\n<li>\n<p>To host Ollama using your machine’s GPU</p>\n</li>\n</ol>\n<pre class=\"codehilite\"><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile gpu up\n</code></pre>\n<ul>\n<li>To host Ollama using your machine’s CPU</li>\n</ul>\n<pre class=\"codehilite\"><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile cpu up\n</code></pre>\n<ul>\n<li>To host Ollama using your machine’sNPU</li>\n</ul>\n<pre class=\"codehilite\"><code>git clone https://github.com/n8n-io/self-hosted-ai-starter-kit.git\ncd self-hosted-ai-starter-kit\ndocker compose --profile npu up\n</code></pre>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/researchgraph.org_wp-admin_post.php_post2056actioneditHighRes-1-1-1024x598.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>After this stage, your n8n self hosted AI starter kit which includes: Ollama, Qdrant and Postgress should be available to be used in your n8n environment. Next, before you can actually create an LLM pipeline, you would need to download said LLM through Ollama’s website -&gt; <a href=\"https://ollama.com/?ref=hub.researchgraph.org\">Ollama website</a> as shown below. <strong>Before that</strong> however, make sure to download Ollama from the website first for Windows of course!</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-10-1024x550.png\"/></p>\n<figcaption>This image is sourced from <a href=\"https://www.datacamp.com/tutorial/local-ai?ref=hub.researchgraph.org\">Ollama </a></figcaption>\n</figure>\n<ol>\n<li>Here in Ollama’s website, you can select any LLM model that is available. For this article’s example, I tried the qwen2.5:1.5b model (1.5b means 1.5 billion parameters) as my LLM. To do this, you need to go to the Search models bar, and search for the model itself as shown below</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-11-1024x551.png\"/></p>\n<figcaption>This image is sourced from <a href=\"https://www.datacamp.com/tutorial/local-ai?ref=hub.researchgraph.org\"> Ollama </a></figcaption>\n</figure>\n<ol>\n<li>And then, select the model version that you want. For my case, again, it’s the 1.5 billion parameter. After you select the model version that you want, copy the text on the right side (i.e., in this case, “ollama run qwen2.5:1.5b”). Afterwards, paste it into your Docker terminal.</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-12-1024x552.png\"/></p>\n<figcaption>This image is sourced from <a href=\"https://ollama.com/library/qwen2.5:1.5b?ref=hub.researchgraph.org\">Ollama Qwen2.5 library </a> </figcaption>\n</figure>\n<ol>\n<li>After you select the model version that you want, copy the text on the right side (i.e., in this case, “ollama run qwen2.5:1.5b”). Afterwards, paste it into your Docker terminal and it will pull the model to Docker.</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-13-1024x602.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>After the installation is finished, all of the set ups have been finished and you can start making LLM pipelines in n8n! You can go back to n8n and make a new workflow. Afterwards, you can use the installed LLM model from Ollama for your n8n pipeline. An example is as such:</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-14-1024x544.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<ol>\n<li>In the example above, it is a basic LLM chain in n8n. The Basic LLM Chain node in n8n integrates large language models into workflows, taking text input, sending it to an LLM for processing, and generating a natural language output for use in subsequent nodes. To set an LLM to be used on this pipeline, you need to click the + icon on Model and select the model that you want to choose, for our example, it is qwen2.5:1.5b, as shown below.</li>\n</ol>\n<figure>\n<p><img alt=\"Image\" src=\"https://hub.researchgraph.org/content/images/2025/05/image-15-1024x502.png\"/></p>\n<figcaption>This image was generated by the author</figcaption>\n</figure>\n<p>And that should be it! You have now made your own LLM pipeline in n8n. You can build upon this current workflow and made cooler feature like for example, make the LLM pipeline to send emails, book calendar events, and many more! Happy building engineers!</p>\n\n<h2>References</h2>\n<ul>\n<li>\n<p><a href=\"https://www.docker.com/?ref=hub.researchgraph.org\">https://www.docker.com</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/n8n-io/self-hosted-ai-starter-kit?ref=hub.researchgraph.org\">https://github.com/n8n-io/self-hosted-ai-starter-kit</a></p>\n</li>\n<li>\n<p><a href=\"https://ollama.com/?ref=hub.researchgraph.org\">https://ollama.com</a></p>\n</li>\n<li>\n<p><a href=\"https://ollama.com/library/qwen2.5:1.5b?ref=hub.researchgraph.org\">https://ollama.com/library/qwen2.5:1.5b</a></p>\n</li>\n</ul>\n<!--kg-card-end: html-->\n<div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/how-to-install-n8n-and-create-an-llm-pipeline-using-ollama-docker-6459fe9ca181?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-05-19-134128.png","state":"findable"},{"id":"https://doi.org/10.59350/pk9ce-m8476","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0007-1385-3343","type":"Person","contributorRoles":["Author"],"givenName":"Yao","familyName":"Chen"}],"date":{"published":"2025-04-28T16:00:00","updated":"2025-05-15T11:17:13"},"descriptions":[{"description":"Introduction This article presents my hands-on experience with <strong> Qwen2.5-MAX </strong> , an advanced open-source large language model developed by Alibaba. I set up and explored this model on my personal computer to evaluate its capabilities against GPT-4.5, a recognised leader in AI technology.","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/pk9ce-m8476.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/pk9ce-m8476.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/pk9ce-m8476.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/pk9ce-m8476.xml"}],"identifiers":[{"identifier":"094208ee-0fc7-494a-9446-45b1253b26ec","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/pk9ce-m8476","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"What is Qwen2.5-MAX: A Practical Exploration And Personal Experience"}],"url":"https://hub.researchgraph.org/what-is-qwen2-5-max-a-practical-exploration-and-personal-experience/","content":"<h2 id=\"introduction\">Introduction</h2><p>This article presents my hands-on experience with&nbsp;<strong>Qwen2.5-MAX</strong>, an advanced open-source large language model developed by Alibaba. I set up and explored this model on my personal computer to evaluate its capabilities against GPT-4.5, a recognised leader in AI technology. Through practical tests involving diverse prompts, this exploration provides insights into Qwen2.5-MAX’s strengths and comparative performance in technical, creative, and logical reasoning tasks.</p><p><strong>Qwen2.5-Max</strong>&nbsp;is the most powerful version of the Qwen2.5 series, developed by Alibaba Cloud. It represents the highest-capacity model variant, designed for tasks requiring advanced reasoning, creative generation, and deep contextual understanding. With&nbsp;<strong>72 billion parameters</strong>, Qwen2.5-Max is pretrained on a massive 18-trillion-token multilingual and multi-domain dataset, supporting context windows of up to&nbsp;<strong>128,000 tokens</strong>.</p><h2 id=\"installation-and-setup\">Installation and Setup</h2><h3 id=\"step1-install-ollama-platform\">Step1: Install Ollama Platform</h3><p>Because Qwen2.5-MAX is not free and open source, I will provide you with guidance on how to deploy Qwen2.5 locally. I installed&nbsp;<strong>Qwen2.5</strong>&nbsp;on my personal computer using the&nbsp;<strong>Ollama platform</strong>, which provided an efficient, user-friendly environment for deploying and managing local AI models. Ollama simplified the installation process significantly, allowing rapid setup and immediate testing.</p><p>Download Ollama from&nbsp;<a href=\"https://ollama.com/?ref=hub.researchgraph.org\">https://ollama.com</a>, choose the appropriate version according to your operating system.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/ollama-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1050\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/ollama-2.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/ollama-2.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/ollama-2.png 1600w, https://hub.researchgraph.org/content/images/size/w2400/2025/05/ollama-2.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><h4 id=\"step2-install-qwen25-locally\">Step2: Install Qwen2.5 locally</h4><p>Find the models list on Ollama website, And you can find Qwen2.5 in the list.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/ollamamod-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1384\" height=\"702\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/ollamamod-1.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/ollamamod-1.png 1000w, https://hub.researchgraph.org/content/images/2025/05/ollamamod-1.png 1384w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Qwen2.5 offers various model sizes ranging from 0.5B to 72B parameters, allowing users to choose the most suitable version based on their device’s available resources. For example, the 32B variant, which I selected, requires approximately 20GB of storage space and offers a good balance between performance and resource demand.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/qwen.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1042\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/qwen.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/qwen.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/qwen.png 1600w, https://hub.researchgraph.org/content/images/size/w2400/2025/05/qwen.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/list-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1836\" height=\"1192\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/list-1.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/list-1.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/list-1.png 1600w, https://hub.researchgraph.org/content/images/2025/05/list-1.png 1836w\" sizes=\"(min-width: 720px) 720px\"></figure><p>You can copy the command and run it in your terminal</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/60.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"974\" height=\"278\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/60.png 600w, https://hub.researchgraph.org/content/images/2025/05/60.png 974w\" sizes=\"(min-width: 720px) 720px\"></figure><p>After you download the model to your local machine, you can check using&nbsp;<strong>ollama list</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/61-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"972\" height=\"170\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/61-2.png 600w, https://hub.researchgraph.org/content/images/2025/05/61-2.png 972w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Then you can use&nbsp;<strong>ollama run qwen2.5:7b</strong>&nbsp;to chat with the model</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/62-2.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1106\" height=\"142\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/62-2.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/62-2.png 1000w, https://hub.researchgraph.org/content/images/2025/05/62-2.png 1106w\" sizes=\"(min-width: 720px) 720px\"></figure><p>Also you can use the Qwen-Max model on the official website&nbsp;<a href=\"https://chat.qwen.ai/c/299d52cc-950a-43a5-8a22-64b010246a6d?ref=hub.researchgraph.org\">https://chat.qwen.ai</a></p><h2 id=\"comparative-analysis-using-prompts\">Comparative Analysis Using Prompts</h2><p>To deeply evaluate the practical performance of&nbsp;<strong>Qwen2.5-Max</strong>, I conducted several tests comparing its responses to those of&nbsp;<strong>GPT-4.5</strong>&nbsp;across multiple types of tasks. Below are the prompts used, followed by a summary of the observed performance differences.</p><h3 id=\"logical-reasoning\">Logical Reasoning</h3><p><strong>Prompt: </strong>“Three friends, Alex, Bob, and Carol, have apples. Alex has three times as many apples as Bob, and Carol has half as many apples as Alex. If Bob has 4 apples, how many apples do they have in total?”</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/55-2048x1218.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1189\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/55-2048x1218.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/55-2048x1218.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/55-2048x1218.png 1600w, https://hub.researchgraph.org/content/images/2025/05/55-2048x1218.png 2048w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Answer by GPT4.5</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/longshot20250422101848-2-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1994\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/longshot20250422101848-2-1.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/longshot20250422101848-2-1.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/longshot20250422101848-2-1.png 1600w, https://hub.researchgraph.org/content/images/2025/05/longshot20250422101848-2-1.png 2168w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Answer by Qwen 2.5</span></figcaption></figure><p>We can see that both models got the correct results, but we can find that Qwen 2.5 has an extra step to define variables. I think this shows that Qwen 2.5 has stronger generalisation ability, that is, it can solve similar problems of the same type, while GPT 4.5 focuses more on the current problem.</p><h3 id=\"concept-explanation\">Concept Explanation</h3><p><strong>Prompt: </strong>“Explain quantum computing in simple terms within 200 words”</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/56.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1774\" height=\"1012\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/56.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/56.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/56.png 1600w, https://hub.researchgraph.org/content/images/2025/05/56.png 1774w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Answer by GPT4.5</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/longshot20250422104053-2-1.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1854\" height=\"1236\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/longshot20250422104053-2-1.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/longshot20250422104053-2-1.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/longshot20250422104053-2-1.png 1600w, https://hub.researchgraph.org/content/images/2025/05/longshot20250422104053-2-1.png 1854w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">Answer by Qwen 2.5</span></figcaption></figure><p>Both models provided good answers to one of the latest scientific and technological concepts.</p><h3 id=\"solving-a-practical-coding-task\">Solving a practical coding task</h3><p><strong>1. Art painting task:&nbsp;</strong>The first<strong> </strong>task is about generating a dynamic art painting. Here is the prompt:</p><pre><code>Write a program to generate a dynamic art painting. Multiple points are randomly generated on the canvas. These points will move according to certain rules (such as random walk, mutual repulsion, following the mouse, etc.) and leave tracks. Finally, a dynamic art work that changes over time is generated.</code></pre><p>Here is the result given by Qwen2.5-Max:</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250429201018_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250429201018.mp4\" poster=\"https://img.spacergif.org/v1/2964x1532/0a/spacer.png\" width=\"2964\" height=\"1532\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250429201018_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:37</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>And here is the result given by GPT-o4-mini-high;</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250429201955_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250429201955.mp4\" poster=\"https://img.spacergif.org/v1/2960x1572/0a/spacer.png\" width=\"2960\" height=\"1572\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250429201955_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:23</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>We can see that they both completed the task successfully, but in my opinion, Qwen2.5-Max’s result is more in line with the requirement of “art painting”. The result given by gpt is very messy, just some randomly generated points and lines. So I think Qwen2.5-Max is the one with better performance.</p><p><strong>2. 2D physics simulation system task</strong>: The second task is about a 2D physical system. Here is the prompt:<br></p><pre><code># Python 2D physics simulation system test questions - cross-rotating regular hexagons\n\n## Basic requirements\nDesign a 2D physics simulation system to implement the following specific scenarios:\n\n### Geometric system:\n- Two regular hexagons of the same size, cross-overlapping each other\n- The regular hexagon on the left rotates counterclockwise (fixed speed)\n- The regular hexagon on the right rotates clockwise (fixed speed)\n- The cross-overlapping part of the two regular hexagons accounts for about half of their respective areas\n- The initial position of a red ball is at the center of the overlapping area of ​​the two regular hexagons\n\n### Physical characteristics:\n- Implement basic Newtonian mechanics (gravity, collision)\n- The collision between the ball and the boundary of the regular hexagon needs to consider the influence of angular momentum (the rotating surface applies tangential force to the ball)\n- The ball must have an appropriate elastic coefficient so that it can continue to bounce in the overlapping area\n- The direction of gravity is fixed downward\n- The ball cannot fall out or pop out of the cross-overlapping part of the two regular hexagons\n\n### Operation mechanism:\n- The system automatically runs the physical simulation after startup\n- Two regular hexagons rotate towards each other at a fixed speed\n- The red ball moves in the overlapping area under the influence of collision, gravity and angular momentum of the rotating surface\n\n### Visualization:\n- Use any Python graphics library (such as Pygame, Pyglet or Tkinter)\n- The regular hexagon on the left is represented by a blue line\n- The regular hexagon on the right is represented by a green line\n- The overlapping area is displayed in different colors (such as purple or light gray)\n- Display the movement trajectory of the red ball (the trajectory gradually disappears)\n- Optional display of current physical parameters (such as ball speed, acceleration)\n\n## Technical requirements:\n- Correctly implement the creation and rotation of geometric shapes\n- Accurately calculate the collision detection between the regular hexagon boundary and the ball\n- Correctly handle the influence of the rotating surface on the angular momentum of the ball\n- Maintain the physical stability of the system so that the simulation can run for a long time\n\n## Submission requirements:\n- Complete Python code, including necessary comments\n- A brief document describing the implementation ideas\n- Explanation of key physical algorithms\n\nMake sure the red ball can continue to move in the overlapping area of ​​two rotating regular hexagons\nand be affected by two regular hexagons rotating in opposite directions, showing interesting physical motion trajectories.</code></pre><p>This time Qwen2.5-Max successfully meets all the requirements but GPT-o4-mini-high failed the core part of the task.</p><p>Here is the result by GPT-o4-mini-high:</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/Untitled-1-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/Untitled-1-1.mp4\" poster=\"https://img.spacergif.org/v1/1618x1234/0a/spacer.png\" width=\"1618\" height=\"1234\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/Untitled-1-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:05</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>Here is the result by Qwen2.5-Max:</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250429233505-1_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250429233505-1.mp4\" poster=\"https://img.spacergif.org/v1/1216x1256/0a/spacer.png\" width=\"1216\" height=\"1256\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250429233505-1_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:30</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><h2 id=\"conclusion\">Conclusion</h2><p>After comparing Qwen2.5-Max with other mainstream models like GPT-4o, I found that for&nbsp;<strong>basic and straightforward tasks</strong>, such as factual Q&amp;A or simple logical problems,&nbsp;<strong>there is little noticeable difference</strong>&nbsp;in performance between them. However, when it comes to&nbsp;<strong>more complex coding tasks or advanced reasoning problems</strong>,&nbsp;<strong>Qwen2.5-Max consistently demonstrated stronger capabilities</strong>, producing more accurate and structured responses. This makes it a highly competitive choice, especially for developers and researchers seeking a powerful open-source alternative that can run locally with sufficient hardware support.</p><p>Ultimately, my experience with Qwen2.5-Max highlights the impressive progress of open-source language models in recent years. Its ability to perform on par with, and in some cases outperform, commercial models like GPT-4o—especially in technical domains—demonstrates its real-world applicability. With flexible deployment via platforms like Ollama and support for various model sizes, Qwen2.5 provides users with both power and adaptability. For anyone interested in AI development, local experimentation, or cost-effective alternatives to commercial APIs, Qwen2.5-Max is undoubtedly worth exploring.</p>","image":"https://hub.researchgraph.org/content/images/2025/05/featuredimage-4.png","state":"findable"},{"id":"https://doi.org/10.59350/9x80h-ys736","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0007-1385-3343","type":"Person","contributorRoles":["Author"],"givenName":"Yao","familyName":"Chen"}],"date":{"published":"2025-04-28T08:28:00","updated":"2025-06-13T16:11:13"},"descriptions":[{"description":"This article explores and analyses the paper ‘Can AI have common sense? Finding out will be key to achieving machine intelligence’ by Mayank Kejriwal and colleagues. The paper was published in 2024. For the open-source version of this blog post, please click this&nbsp;link.","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/9x80h-ys736.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/9x80h-ys736.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/9x80h-ys736.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/9x80h-ys736.xml"}],"identifiers":[{"identifier":"65951972-a45a-4e77-a0e7-bfe7103845ec","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/9x80h-ys736","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://doi.org/10.1038/d41586-024-03262-z","unstructured":"Kejriwal, M., Santos, H., Mulvehill, A. M., Shen, K., McGuinness, D. L., &amp; Lieberman, H. (2024). Can AI have common sense? Finding out will be key to achieving machine intelligence. <i>Nature</i>, <i>634</i>(8033), 291–294. https://doi.org/10.1038/d41586-024-03262-z"}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"Exploring Machine Common Sense"}],"url":"https://hub.researchgraph.org/exploring-machine-common-sense/","content":"<h1 id=\"introduction\">Introduction</h1><p>As artificial intelligence (AI) continues to evolve, one of the most interesting challenges researchers face is determining whether machines can possess something like human consciousness — or, at the very least, human-like reasoning. While we’ve made remarkable strides with models like GPT-4, which powers ChatGPT, a fundamental question still remains:&nbsp;<strong>Can machines develop common sense</strong>?</p><p>For humans, common sense is our intuitive understanding of the world around us — knowing that glass breaks easily, chairs are for sitting, and that vegetarian friends may not like meat-based meals. For machines, common sense is a more difficult goal to achieve. Despite their ability to process large amounts of data and answer questions quickly and accurately, AI systems such as LLMs (large language models) often fail at some basic reasoning tasks, which raises an important question: How do we measure whether AI truly understands the world like humans?</p><h1 id=\"current-status-ai%E2%80%99s-shortcomings\">Current Status: AI’s Shortcomings</h1><p>Over the past few years, LLMs like GPT-4 have given us amazing capabilities. They can generate text, pass medical and bar exams, and even compose poetry. However, they still struggle with tasks that require basic reasoning. For example, when asked a simple question: “Riley felt pain. How did Riley feel afterwards?” the model’s best answer was “aware,” not “painful.” This disconnect suggests that while the model is able to mimic human responses, it doesn’t truly understand emotions or physical sensations — core aspects of common sense.</p><p>These failures are not trivial. They reveal a deeper problem: most AI systems today are very good at memorising facts, but often fail at reasoning tasks that require flexible adaptation to real-world situations. Common sense often involves dealing with ambiguity, coping with uncertainty, and making new decisions based on experience. This is where humans can intuitively adjust their reasoning based on context, and AI still can’t replicate that.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/0-PGGHf1chg8OHrrc7.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"778\" height=\"262\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/0-PGGHf1chg8OHrrc7.png 600w, https://hub.researchgraph.org/content/images/2025/05/0-PGGHf1chg8OHrrc7.png 778w\" sizes=\"(min-width: 720px) 720px\"><figcaption><span style=\"white-space: pre-wrap;\">How does AI reach its conclusions?</span></figcaption></figure><h1 id=\"why-existing-testing-methods-are-insufficient\">Why existing testing methods are insufficient</h1><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*J8ipE3nMt9SiCb0k.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"208\"><figcaption><span style=\"white-space: pre-wrap;\">Example of a Multiple Choice Question AI is tested on</span></figcaption></figure><p>Traditionally, AI’s common sense is tested with multiple-choice questions (MCQs). These tests may seem adequate, but in reality, they don’t really measure a machine’s reasoning ability. While these questions assess a model’s factual recall, they don’t assess whether the model understands the reasoning behind its response. For example, a machine may know that a cup of coffee gets cold when left out, but does it understand the concepts of heat transfer and thermal balance?</p><p>More importantly, MCQs often overlook the subtlety and context-dependency of common sense. Humans are able to intuitively process these questions when faced with these contexts. For example: ‘Should we serve cake when they visit? I think Lina and Michael are on a diet.’ But if more information is added — like, ‘But I know they have a cheat day’ — the context of the decision changes completely. For AI, handling this kind of ambiguity and changing context is a huge challenge.</p><h1 id=\"the-importance-of-reasoning-more-than-just-memorising-facts\">The Importance of Reasoning: More Than Just Memorising Facts</h1><p>For a machine to exhibit common sense, it must do more than just memorise facts — it needs to infer new information from existing knowledge and apply it appropriately to different contexts. This is where many AI systems fail. They excel at pattern recognition and fact retrieval, but they often fail to draw new, plausible conclusions when faced with ambiguous or incomplete information.</p><p>To address this, researchers are pushing for better ways to evaluate AI reasoning. One possible improvement is to&nbsp;<strong>require AI models to explain their reasoning.</strong>&nbsp;If an AI model can logically explain why a cup of coffee cools, rather than simply recalling “because the heat dissipated into the surrounding air,” then we have reason to believe that the model is not just looking up information from its large training corpus, but is reasoning in a meaningful way.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*c-e3P8c_l9Q-T1Nw.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"417\"><figcaption><span style=\"white-space: pre-wrap;\">Reasoning explanation by GPT-4o</span></figcaption></figure><h1 id=\"the-road-ahead-new-common-sense-assessment-benchmarks\">The road ahead: new common sense assessment benchmarks</h1><p>The path to machine common sense is not simple and involves rethinking how we measure reasoning. Current assessment benchmarks are too narrow and often fail to capture human-like reasoning abilities. Researchers argue that AI systems should be assessed on a broad range of reasoning skills, such as understanding physical properties, social interactions, and causal relationships.</p><p>To make substantial progress, we must design theory-driven tests that should not be limited to multiple-choice questions but should focus&nbsp;<strong>on practical tasks that require multi-step reasoning</strong>. For example, planning or strategy tasks, such as collecting energy tokens on a chessboard, may be a good way to test whether an AI can reason in a dynamic environment. Humans may not always find the optimal solution, but we are able to make reasonable decisions based on common sense. The fact that AI still performs poorly in this environment suggests that it still struggles with more complex, real-world problems.</p><h1 id=\"embodied-common-sense-thinking-beyond-language\">Embodied Common Sense: Thinking Beyond Language</h1><p>Another key step in developing machine common sense is to go beyond language. LLMs perform very well on language-related tasks, but true common sense involves understanding the physical world, social cues, and human experience — all of which are not just linguistic constructs.</p><p><strong>Embodied common sense</strong>&nbsp;means that machines can not only “understand” the linguistic description of the world, but also perceive and interact with the real world. For example, understanding and responding to physical conditions in the environment (such as the movement of objects, weather changes, etc.), as well as understanding and responding to the emotions or behaviours of others. Embodied common sense is essential to building machines that can make decisions in complex environments. Simply put, the machine not only needs to “know” what common sense is, but also be able to interact with the environment and understand the actual world it lives in.</p><p>Embodied common sense is necessary for machines to truly demonstrate human-like reasoning abilities. For example, as babies grow up, they first learn how to understand the world by sensing their environment and interacting with people. Similarly, machines need to learn common sense by sensing and interacting with the physical world. Currently, machines are in the early stages of acquiring this “physical intelligence,” just like babies, but progress is being made.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://miro.medium.com/v2/resize:fit:840/0*VrwJObdJmS9a0H2s.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"700\" height=\"451\"><figcaption><span style=\"white-space: pre-wrap;\">AI is evolving like humans</span></figcaption></figure><h1 id=\"conclusion-the-future-of-machine-common-sense\">Conclusion: The Future of Machine Common Sense</h1><p>The road to machine common sense is long and challenging. Despite impressive progress in AI, we still have a long way to go to create machines that can exhibit human-like consciousness and reasoning. As AI becomes more commonplace in a variety of fields, common sense reasoning will be key to ensuring the reliability and ethics of these systems.</p><p>The future of AI is not just about creating systems that can recall facts, but about creating systems that can reason, understand, and adapt to a complex world. Advances in this process may not only improve the capabilities of AI systems but also provide deeper cognition for ourselves. Machine common sense may not only revolutionise technology, but also help us more deeply understand the nature of human thinking and reasoning.</p><p>In the long run, the science of machine common sense may not only drive a technological revolution, but may also provide us with deep insights into how we think and reason.</p><h1 id=\"references\">References</h1><ul><li>Mayank Kejriwal, Santos, H., Mulvehill, A.M., Shen, K., McGuinness, D.L. and Lieberman, H. (2024b). Can AI have common sense? Finding out will be key to achieving machine intelligence. <em>Nature. </em><a href=\"https://doi.org/10.1038/d41586-024-03262-z?ref=hub.researchgraph.org\">https://doi.org/10.1038/d41586-024-03262-z</a>.</li></ul><div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/exploring-machine-common-sense-8808ef5856ab?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/machine_common_sense.png","state":"findable"},{"id":"https://doi.org/10.59350/ft1nd-0rt41","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0009-3354-7794","type":"Person","contributorRoles":["Author"],"givenName":"Immanuel Alvaro","familyName":"Bhirawa"}],"date":{"published":"2025-04-28T08:07:00","updated":"2025-05-15T10:11:00"},"descriptions":[{"description":"Introduction In February 2025, xAI unveiled Grok 3, a highly anticipated AI model touted as the “smartest AI in the world” (xAI). Developed to advance scientific discovery and deepen our understanding of the universe, Grok 3 promises superior reasoning, real-time data processing, and versatile applications.","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/ft1nd-0rt41.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/ft1nd-0rt41.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/ft1nd-0rt41.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/ft1nd-0rt41.xml"}],"identifiers":[{"identifier":"67be2b23-690b-4536-81d2-37e6d3f3daae","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/ft1nd-0rt41","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"references":[{"id":"https://x.ai/news/grok-3","unstructured":"xAI Official Grok 3 Beta Announcement"},{"id":"https://www.cnet.com/tech/services-and-software/musks-xai-launches-grok-3-heres-what-you-need-to-know/","unstructured":"CNET: Musk’s xAI Launches Grok 3 Overview"},{"id":"https://techcrunch.com/2025/02/17/elon-musks-ai-company-xai-releases-its-latest-flagship-ai-grok-3/","unstructured":"TechCrunch: xAI Releases Flagship Grok 3 Model"},{"id":"https://anthemcreation.com/en/artificial-intelligence/grok-3-xai-pushes-the-borderlines-artificial-intelligence/","unstructured":"Anthem Creation: Grok 3 Technical and Ethical Analysis"},{"id":"https://lifearchitect.ai/whats-in-grok/","unstructured":"Life Architect: Comprehensive Grok Models Analysis"},{"id":"https://www.helicone.ai/blog/grok-3-benchmark-comparison","unstructured":"Helicone: Grok 3 Benchmark Comparison Review"},{"id":"https://latenode.com/blog/grok-3-review","unstructured":"Latenode: Grok 3 User Testing Experience"},{"id":"https://writesonic.com/blog/grok-3-review","unstructured":"Writesonic: Grok 3 Prompt Testing Review"},{"id":"https://www.tomsguide.com/ai/i-tested-grok-3-with-5-prompts-heres-what-i-like-and-dont-like-about-this-chatbot","unstructured":"Caswell, A. (2025). I just tested the new Grok-3 with 5 prompts &mdash; here&rsquo;s what I like and don&rsquo;t like about this chatbot. In <i>Tom's Guide</i>. Tom's Guide. https://www.tomsguide.com/ai/i-tested-grok-3-with-5-prompts-heres-what-i-like-and-dont-like-about-this-chatbot"},{"id":"https://decrypt.co/306722/grok-3-review-how-elon-musks-ai-compares-to-chatgpt-claude-deepseek-and-gemini","unstructured":"Lanz, J. A. (2025). Grok-3 Review: How Elon Musk’s AI Compares to ChatGPT, Claude, DeepSeek and Gemini - Decrypt. In <i>Decrypt</i>. Decrypt. https://decrypt.co/306722/grok-3-review-how-elon-musks-ai-compares-to-chatgpt-claude-deepseek-and-gemini"},{"id":"https://techcrunch.com/2025/04/09/elon-musks-ai-company-xai-launches-an-api-for-grok-3/","unstructured":"TechCrunch: xAI Launches Grok 3 API"},{"id":"https://en.wikipedia.org/wiki/Grok_(chatbot)","unstructured":"projects, C. to W. (2023). <i>Grok (chatbot)</i>. Wikimedia Foundation, Inc. https://en.wikipedia.org/wiki/Grok_(chatbot)"}],"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"Exploring Grok 3: xAI’s homebrew Artificial Intelligence"}],"url":"https://hub.researchgraph.org/exploring-grok-3-xais-homebrew-artificial-intelligence/","content":"<h2 id=\"introduction\">Introduction</h2><p>In February 2025, xAI unveiled Grok 3, a highly anticipated AI model touted as the “smartest AI in the world” (xAI). Developed to advance scientific discovery and deepen our understanding of the universe, Grok 3 promises superior reasoning, real-time data processing, and versatile applications. This literature review synthesizes insights from 10 articles and blog posts to explore what Grok 3 is, its key features, performance, use cases, and user experiences, offering a balanced perspective on its capabilities and limitations.</p><h2 id=\"what-is-grok-3\">What is Grok 3?</h2><p>Grok 3 is xAI’s third-generation AI model, launched on February 17, 2025, as a successor to Grok 2. According to xAI, it was trained on the Colossus supercluster with 10 times the computational power of previous models, utilizing 200,000 NVIDIA H100 GPUs (Anthem Creation). Elon Musk, xAI’s founder, claims Grok 3 is “an order of magnitude more powerful” than its predecessor, designed to outperform competitors like OpenAI’s GPT-4o and Google’s Gemini (CNET). Its training dataset reportedly includes 280 billion multilingual text tokens, 14 million hours of annotated educational videos, and legal archives from 12 countries, though exact details remain undisclosed (Anthem Creation).</p><p>Grok 3 is accessible via X Premium+ subscriptions ($40/month), a standalone SuperGrok subscription ($30/month), or through web and mobile apps (TechCrunch). It also powers features on X, xAI’s social platform, and supports an API for developers, priced at $3 per million input tokens (TechCrunch API).</p><h2 id=\"key-features-and-capabilities\">Key Features and Capabilities</h2><p>Grok 3 stands out for its advanced reasoning, large context window, and innovative tools. Below are its primary features:</p><h3 id=\"reasoning-capabilities\">Reasoning Capabilities</h3><p>Grok 3 employs large-scale reinforcement learning to refine its reasoning, allowing it to “think” for seconds to minutes, correct errors, and explore alternative solutions (xAI). It offers two operational modes:</p><ul><li><strong>Think Mode</strong>: Executes 48 cognitive steps with real-time cross-validation for accurate responses.</li><li><strong>Big Brain Mode</strong>: Utilizes 12.8 TB VRAM per request, achieving 99.4% accuracy on the GPQA benchmark (Anthem Creation).</li></ul><h3 id=\"performance-on-benchmarks\">Performance on Benchmarks</h3><p>Grok 3 demonstrates strong performance across academic benchmarks, as shown in the recreated table below, based on data from Anthem Creation:</p>\n<!--kg-card-begin: html-->\n<table class=\"has-fixed-layout\" style=\"box-sizing: border-box; background-color: transparent; width: 1140px; margin-block-end: 15px; font-size: 0.9em; border-spacing: 0px; border-collapse: collapse; table-layout: fixed;\"><tbody style=\"box-sizing: border-box;\"><tr style=\"box-sizing: border-box;\"><th style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; font-weight: 700; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">Benchmark</th><th style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; font-weight: 700; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">Grok 3</th><th style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; font-weight: 700; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">GPT-4.5</th><th style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; font-weight: 700; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">Gemini Ultra</th></tr><tr style=\"box-sizing: border-box;\"><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">AIME (Maths)</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">92.1%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">89.7%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">88.3%</td></tr><tr style=\"box-sizing: border-box;\"><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">GPQA (Science)</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">94.6%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">91.2%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">89.8%</td></tr><tr style=\"box-sizing: border-box;\"><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">Codex (Python)</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">89.3%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">87.1%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; word-break: break-word;\">85.6%</td></tr><tr style=\"box-sizing: border-box;\"><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">MMLU (General)</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">93.8%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">94.1%</td><td style=\"box-sizing: border-box; padding: 0.5em; line-height: 1.5; vertical-align: top; border: 1px solid; background-color: rgba(128, 128, 128, 0.07); word-break: break-word;\">92.4%</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<p>Grok 3 also achieved an Elo score of 1402 in the Chatbot Arena, indicating strong user preference (xAI). However, it shows 37% lower performance variance across domains, suggesting consistent reliability.</p><p><strong>Additional comparison performance</strong></p><p>For further comparison visualization between Grok and ChatGPT, I have given it a prompt for testing. The prompt being:</p><pre><code>Generate Python code using matplotlib to create a bar chart with the following population data: {'China': 1444216107, 'India': 1393409038, 'United States': 332915073, 'Indonesia': 276361783, 'Pakistan': 225199937}.</code></pre><p>This is the result that was given from&nbsp;<strong>Grok</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://raw.githubusercontent.com/Immanuel-Alvaro-Bhirawa/RGF-ImageStorage/main/image_2025-04-28_130105218.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1191\" height=\"734\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">This image is generated from </em></i><a href=\"https://grok.com/share/bGVnYWN5_8c57194f-e999-445e-ae13-f519e0c3867c?ref=hub.researchgraph.org\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">xAI’s grok-3</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">.</em></i></figcaption></figure><p>And this was the result generated from&nbsp;<strong>ChatGPT</strong></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://raw.githubusercontent.com/Immanuel-Alvaro-Bhirawa/RGF-ImageStorage/main/image_2025-04-28_130746404.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1369\" height=\"796\"><figcaption><i><em class=\"italic\" style=\"white-space: pre-wrap;\">This image is generated from OpenAI’s </em></i><a href=\"https://chatgpt.com/share/680ef3a2-8748-800c-8ec3-c6a033ed59d5?ref=hub.researchgraph.org\"><i><em class=\"italic\" style=\"white-space: pre-wrap;\">ChatGPT 4o</em></i></a><i><em class=\"italic\" style=\"white-space: pre-wrap;\">.</em></i></figcaption></figure><h3 id=\"deepsearch-and-multimodality\">DeepSearch and Multimodality</h3><p>Grok 3’s DeepSearch tool enables real-time web crawling and information synthesis, analysing millions of pages with 82% accuracy compared to Gemini’s 67% (Anthem Creation). Its cognitive multimodality allows it to process text, images, and symbolic reasoning, solving handwritten equations and generating physical simulations (Anthem Creation).</p><h3 id=\"grok-3-mini\">Grok 3 Mini</h3><p>xAI also released Grok 3 Mini, a cost-efficient version for STEM tasks, with performance metrics like 95.8% on AIME’24 when using Think Mode (xAI).</p><h2 id=\"use-cases-and-applications\">Use Cases and Applications</h2><p>Grok 3’s versatility supports a range of applications:</p><ul><li><strong>Scientific Research</strong>: DeepSearch aids in synthesising real-time scientific data, useful for researchers (xAI).</li><li><strong>Coding and Development</strong>: Users report success in code debugging and Python script generation (Latenode).</li><li><strong>Business Analysis</strong>: Grok 3 provides nuanced analyses, such as evaluating AI regulations (Latenode).</li><li><strong>Creative Tasks</strong>: It can write in specific styles (e.g., Hemingway’s voice) and generate images, though creativity is sometimes predictable (Latenode).</li><li><strong>Government Use</strong>: The U.S. Department of Government Efficiency reportedly used Grok 3 for policy work (Wikipedia).</li></ul><p>Potential future applications include brain-machine interfacing, combining local and cloud-based models for cognitive enhancement (Life Architect).</p><h2 id=\"comparing-grok-3-to-other-models\">Comparing Grok 3 to Other Models</h2><p>Grok 3 is positioned as a rival to GPT-4o, Claude 3.5 Sonnet, DeepSeek, and Gemini. While xAI claims superiority, reviews offer mixed perspectives:</p><ul><li><strong>Helicone</strong>&nbsp;notes Grok 3’s 1-million-token context window matches Gemini 2.5 and GPT-4.1, but its real-world performance varies (Helicone).</li><li><strong>Decrypt</strong>&nbsp;found Grok 3 excels in reasoning but doesn’t consistently outperform competitors across all tasks (Decrypt).</li><li><strong>Lifehacker</strong>&nbsp;argues that Grok 3’s improvements are incremental, not revolutionary, and its price hike may not be justified (Lifehacker).</li></ul><h2 id=\"others%E2%80%99-experiences-with-grok-3\">Others’ Experiences with Grok 3</h2><p>User reviews highlight both strengths and weaknesses:</p><ul><li><strong>Latenode</strong>&nbsp;praises Grok 3’s “absurdly fast” responses and 89.7% sarcasm detection accuracy but notes predictable creative outputs and cautious moral guardrails (Latenode).</li><li><strong>Writesonic</strong>&nbsp;tested over 100 prompts, confirming Grok 3’s contextual depth but identifying quirks like occasional inaccuracies (Writesonic).</li><li><strong>Tom’s Guide</strong>&nbsp;appreciated Grok 3’s nuanced responses to complex prompts, such as economic analyses, but criticised its use of assumptions over definitive data (Tom’s Guide).</li><li><strong>Lifehacker</strong>&nbsp;found Grok 3 prone to hallucinations, similar to other models, and questioned its value given the cost (Lifehacker).</li><li><strong>Unite.AI</strong>&nbsp;lauded Grok 3’s near-human reasoning but noted its reliance on a costly infrastructure (Unite.AI).</li></ul><h2 id=\"ethical-and-technical-challenges\">Ethical and Technical Challenges</h2><p>Some sources raise concerns about Grok 3’s transparency and ethical alignment. The model’s estimated 1.8 trillion parameters and undisclosed synthetic data share spark reproducibility issues (Anthem Creation). Additionally, its adaptive personalisation, which adjusts responses based on user interaction history, raises privacy questions (Anthem Creation).</p><h2 id=\"conclusion\">Conclusion</h2><p>Grok 3 represents a significant step forward for xAI, with its advanced reasoning, real-time data access, and strong benchmark performance positioning it as a formidable competitor in the AI landscape. However, mixed user reviews and concerns about cost and transparency suggest it may not fully meet the hype for all users. For researchers, developers, and enthusiasts, Grok 3 offers exciting possibilities, but its value depends on specific use cases and budget considerations. As xAI continues to refine the model, it will be fascinating to see how Grok 3 shapes the future of AI.</p><h2 id=\"references\"><strong>References</strong></h2><ul><li><a href=\"https://x.ai/news/grok-3?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">xAI Official Grok 3 Beta Announcement</a></li><li><a href=\"https://www.cnet.com/tech/services-and-software/musks-xai-launches-grok-3-heres-what-you-need-to-know/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">CNET: Musk’s xAI Launches Grok 3 Overview</a></li><li><a href=\"https://techcrunch.com/2025/02/17/elon-musks-ai-company-xai-releases-its-latest-flagship-ai-grok-3/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">TechCrunch: xAI Releases Flagship Grok 3 Model</a></li><li><a href=\"https://anthemcreation.com/en/artificial-intelligence/grok-3-xai-pushes-the-borderlines-artificial-intelligence/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Anthem Creation: Grok 3 Technical and Ethical Analysis</a></li><li><a href=\"https://lifearchitect.ai/whats-in-grok/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Life Architect: Comprehensive Grok Models Analysis</a></li><li><a href=\"https://www.helicone.ai/blog/grok-3-benchmark-comparison?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Helicone: Grok 3 Benchmark Comparison Review</a></li><li><a href=\"https://latenode.com/blog/grok-3-review?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Latenode: Grok 3 User Testing Experience</a></li><li><a href=\"https://writesonic.com/blog/grok-3-review?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Writesonic: Grok 3 Prompt Testing Review</a></li><li><a href=\"https://www.tomsguide.com/ai/i-tested-grok-3-with-5-prompts-heres-what-i-like-and-dont-like-about-this-chatbot?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Tom’s Guide: Grok 3 Prompt-Based Chatbot Review</a></li><li><a href=\"https://decrypt.co/306722/grok-3-review-how-elon-musks-ai-compares-to-chatgpt-claude-deepseek-and-gemini?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Decrypt: Grok 3 Comparison with Other AI Models</a></li><li><a href=\"https://techcrunch.com/2025/04/09/elon-musks-ai-company-xai-launches-an-api-for-grok-3/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">TechCrunch: xAI Launches Grok 3 API</a></li><li><a href=\"https://en.wikipedia.org/wiki/Grok_%28chatbot%29?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Wikipedia: Grok Chatbot Overview</a><a href=\"https://www.unite.ai/grok-3-review/?ref=hub.researchgraph.org\" rel=\"noreferrer noopener\">Unite.AI: Grok 3 Review and Capabilities</a></li></ul>","image":"https://hub.researchgraph.org/content/images/2025/05/image_2025-04-28_131126607.png","state":"findable"},{"id":"https://doi.org/10.59350/7vzbj-46v27","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0006-5800-1198","type":"Person","contributorRoles":["Author"],"givenName":"Aditya","familyName":"Arora"}],"date":{"published":"2025-04-28T06:26:00","updated":"2025-05-15T09:14:30"},"descriptions":[{"description":"Introduction Vision-enabled AI models have rapidly evolved to become essential tools across numerous applications, from content moderation to image analysis and multimodal reasoning. Cohere's recent entry into this space with their Aya Vision model promises to deliver competitive capabilities in the increasingly crowded market of multimodal AI systems.","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/7vzbj-46v27.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/7vzbj-46v27.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/7vzbj-46v27.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/7vzbj-46v27.xml"}],"identifiers":[{"identifier":"ec21f47b-88b2-4ef1-abd8-83872e975515","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/7vzbj-46v27","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"Exploring Aya Vision: Cohere's Multimodal Model and Its Comparison with GPT"}],"url":"https://hub.researchgraph.org/exploring-aya-vision-coheres-multimodal-model-and-its-comparison-with-gpt/","content":"<h2 id=\"introduction\">Introduction</h2><p>Vision-enabled AI models have rapidly evolved to become essential tools across numerous applications, from content moderation to image analysis and multimodal reasoning. Cohere's recent entry into this space with their Aya Vision model promises to deliver competitive capabilities in the increasingly crowded market of multimodal AI systems.</p><p>In this blog post, I'll share my hands-on experience testing Aya Vision (32B model) against GPT-4o, focusing on several key areas critical for real-world applications. Rather than relying on marketing claims or theoretical specifications, this analysis is based on direct testing with identical prompts and images across both models.</p><h2 id=\"what-is-aya-vision\">What is Aya Vision?</h2><p>Aya Vision, part of Cohere’s Aya family, aims to make generative AI accessible across languages and modalities. It’s available in two sizes—8 billion parameters (Aya Vision 8B) and 32 billion parameters (Aya Vision 32B)—and is optimised for vision-language tasks. As an open-weight model, it’s freely accessible for non-commercial research via platforms like Hugging Face and Kaggle. You can also access it using their <a href=\"https://dashboard.cohere.com/playground/chat?ref=hub.researchgraph.org\">Playground</a> platform. Supporting 23 languages and covering half the world’s population, Aya Vision is designed for tasks like image captioning, visual question answering, text generation, and translation, making it a versatile tool for global applications.</p><h2 id=\"testing-methodology\">Testing Methodology</h2><p>My testing approach involved challenging both models with identical images and prompts across several categories:</p><ol><li><strong>Technical Code Analysis</strong>: Evaluating how well each model can interpret and explain programming code in images</li><li><strong>Basic Image Recognition</strong>: Testing fundamental object identification capabilities</li><li><strong>Visual Reasoning</strong>: Assessing the ability to make accurate inferences about visual information</li><li><strong>OCR and Text Interpretation</strong>: Examining how effectively the models can read and understand text in images</li><li><strong>Object Counting</strong>: Testing precision in counting and identifying multiple objects</li><li><strong>Multilingual Capabilities</strong>: Assessing ability to recognise and translate non-English text</li></ol><h2 id=\"test-results\">Test Results</h2><h3 id=\"1-technical-code-analysis-flutter-code-snippet\">1. Technical Code Analysis: Flutter Code Snippet</h3><p><strong>The Challenge</strong>: Both models were presented with a Flutter (Dart) code snippet and asked to analyse it.</p><p><strong>Prompt</strong>: Analyse the code snippet shown in this image.</p><p><strong>Image used</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/flutter.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1422\" height=\"1496\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/flutter.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/flutter.png 1000w, https://hub.researchgraph.org/content/images/2025/05/flutter.png 1422w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>GPT-4V's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-11.32.33-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1978\" height=\"1452\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-11.32.33-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-11.32.33-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-11.32.33-pm.png 1600w, https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-11.32.33-pm.png 1978w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>✅ Correctly identified the language as Dart/Flutter</li><li>✅ Accurately recognised key components (FloatyHead, MaterialApp, StatefulWidget)</li><li>✅ Correctly inferred the likely purpose (floating window/outgoing call notification)</li><li>✅ Noted the red squiggly lines indicating possible errors</li><li>✅ Provided practical suggestions for fixing the code</li><li>✅ Used a well-structured, developer-friendly format</li></ul><p><strong>Aya Vision's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://github.com/AdiAr11/internship-blogs-images/blob/main/week08%20aya%20images/Screenshot%202025-04-27%20at%2011.34.57%E2%80%AFpm.png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1964\" height=\"1028\"></figure><ul><li>❌ Incorrectly identified the language as TypeScript</li><li>❌ Misinterpreted Flutter-specific components as React/Angular</li><li>❌ Created fictitious explanations about non-existent components (e.g., \"sync\" function)</li><li>❌ Mentioned JSX-like syntax which isn't present in the actual code</li><li>❌ Made up components that don't exist in the code (\"PaddedText\", \"View\")</li><li>❌ Exhibited significant hallucination, inventing content not present in the image</li></ul><p><strong>Analysis</strong>: In this test, GPT-4V demonstrated significantly superior technical understanding and accuracy. Aya Vision's response included substantial hallucinations, raising serious concerns about its reliability for technical use cases.</p><h3 id=\"2-basic-image-recognition-eyeglasses\">2. Basic Image Recognition: Eyeglasses</h3><p><strong>The Challenge</strong>: Both models were shown an image of transparent eyeglasses on a white surface and asked to describe what they saw.</p><p><strong>Prompt</strong>: Describe what you see in this image</p><p><strong>Image used</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/glasses.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1497\" height=\"842\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/glasses.jpeg 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/glasses.jpeg 1000w, https://hub.researchgraph.org/content/images/2025/05/glasses.jpeg 1497w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>GPT-4V's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://github.com/AdiAr11/internship-blogs-images/blob/main/week08%20aya%20images/Screenshot%202025-04-27%20at%205.33.36%E2%80%AFpm.png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1784\" height=\"1138\"></figure><ul><li>✅ Correctly identified eyeglasses</li><li>✅ Noted the translucent/light-colored frame</li><li>✅ Described the retro, oversized design</li><li>✅ Mentioned metallic elements on the temples</li><li>✅ Kept the description accurate and concise</li></ul><p><strong>Aya Vision's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-5.33.12-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1250\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-5.33.12-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-5.33.12-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-5.33.12-pm.png 1600w, https://hub.researchgraph.org/content/images/size/w2400/2025/05/Screenshot-2025-04-27-at-5.33.12-pm.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>✅ Correctly identified clear plastic glasses</li><li>✅ Described the rectangular frame with rounded corners</li><li>✅ Noted the minimalist design</li><li>✅ Mentioned the white, textured surface underneath</li><li>✅ Provided accurate details about the glasses' condition</li></ul><p><strong>Analysis</strong>: Both models performed well on this basic image recognition task, with similarly accurate descriptions. This suggests that for straightforward object identification, both models have comparable capabilities.</p><h3 id=\"3-visual-reasoning-food-identification\">3. Visual Reasoning: Food Identification</h3><p><strong>The Challenge</strong>: Both models were shown an image of \"mushroom buns\" in a bamboo steamer basket and asked to identify the dish.</p><p><strong>Prompt: </strong>Can you guess what dish is in the image?</p><p><strong>Image used</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/dim-sum.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1600\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/dim-sum.jpeg 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/dim-sum.jpeg 1000w, https://hub.researchgraph.org/content/images/2025/05/dim-sum.jpeg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>GPT-4V's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.03.50-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1646\" height=\"602\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-6.03.50-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-6.03.50-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-6.03.50-pm.png 1600w, https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.03.50-pm.png 1646w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>✅ Correctly identified the dish as \"mushroom buns\" served at dim sum restaurants</li><li>✅ Explained that they're steamed buns designed to look like mushrooms</li><li>✅ Noted that they typically contain sweet fillings like custard or red bean paste</li><li>✅ Recognised the intentional design mimicking mushroom caps</li><li>✅ Showed cultural knowledge about the dish's popularity</li></ul><p><strong>Aya Vision's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.03.19-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1976\" height=\"970\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-6.03.19-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-6.03.19-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-6.03.19-pm.png 1600w, https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.03.19-pm.png 1976w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>❌ Incorrectly identified the dish as \"char siu bao\" (barbecued pork buns)</li><li>❌ Made incorrect assumptions about the filling being barbecued pork</li><li>❌ Provided irrelevant information about caramelisation</li><li>✅ Correctly identified the bamboo steamer and dim sum connection</li></ul><p><strong>Analysis</strong>: This test revealed significant differences in visual reasoning capabilities. GPT-4V correctly identified the uniquely designed mushroom buns, while Aya Vision misidentified them as a different dim sum dish entirely, demonstrating less refined visual reasoning.</p><h3 id=\"4-text-interpretation-career-document\">4. Text Interpretation: Career Document</h3><p><strong>The Challenge</strong>: Both models were presented with a text-heavy image containing a \"Career Episode\" document and asked to summarise the key points.</p><p><strong>Prompt: </strong>Summarise the key points from the text in this image.</p><p><strong>Image used</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://github.com/AdiAr11/internship-blogs-images/blob/main/week08%20aya%20images/career%20episode.png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1030\" height=\"1346\"></figure><p><strong>GPT-4V's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://github.com/AdiAr11/internship-blogs-images/blob/main/week08%20aya%20images/Screenshot%202025-04-27%20at%206.06.35%E2%80%AFpm.png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1652\" height=\"1416\"></figure><ul><li>✅ Extracted precise details (Flutter Developer Intern at STAGE, July 2022- June 2023)</li><li>✅ Correctly identified the company's focus (Indian regional content, OTT platform)</li><li>✅ Listed specific technical accomplishments (Navigation 1.0 to 2.0 migration, Sound Null Safety implementation)</li><li>✅ Captured challenges faced (limited Flutter resources, remote work communication)</li><li>✅ Organised information in a clear, structured format</li><li>✅ Maintained factual accuracy throughout</li></ul><p><strong>Aya Vision's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.06.08-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"1005\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-6.06.08-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-6.06.08-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-6.06.08-pm.png 1600w, https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.06.08-pm.png 2002w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>❌ Failed to extract specific dates and company details</li><li>❌ Made vague generalisations about \"full-stack application for producing Indian regional content\"</li><li>❌ Incorrectly framed the document as if it were hypothetical, not a real experience</li><li>❌ Referenced non-existent elements (\"Sound-Null Safety platform\" rather than the concept)</li><li>❌ Missed key technical accomplishments and specific challenges</li><li>❌ Created a generic summary with poor factual fidelity to the original text</li></ul><p><strong>Analysis</strong>: GPT-4V demonstrated superior ability to extract, organise, and faithfully represent text information from the image, while Aya Vision provided a vague summary with multiple factual errors and omissions.</p><h3 id=\"5-object-counting-urban-scene-with-cars\">5. Object Counting: Urban Scene with Cars</h3><p><strong>The Challenge</strong>: Both models were shown an image of a street scene with multiple vehicles and asked to count the cars.</p><p><strong>Prompt</strong>: How many cars do you see in this image?</p><p><strong>Image</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/lambo.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1200\" height=\"1600\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/lambo.jpeg 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/lambo.jpeg 1000w, https://hub.researchgraph.org/content/images/2025/05/lambo.jpeg 1200w\" sizes=\"(min-width: 720px) 720px\"></figure><p><strong>GPT-4V's Response</strong>:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.48.05-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"2000\" height=\"904\" srcset=\"https://hub.researchgraph.org/content/images/size/w600/2025/05/Screenshot-2025-04-27-at-6.48.05-pm.png 600w, https://hub.researchgraph.org/content/images/size/w1000/2025/05/Screenshot-2025-04-27-at-6.48.05-pm.png 1000w, https://hub.researchgraph.org/content/images/size/w1600/2025/05/Screenshot-2025-04-27-at-6.48.05-pm.png 1600w, https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.48.05-pm.png 2052w\" sizes=\"(min-width: 720px) 720px\"></figure><ul><li>✅ Identified a total of six cars in the image</li><li>✅ Correctly recognised the white Lamborghini in the foreground</li><li>✅ Noted the black SUV parked behind it</li><li>✅ Identified approximately four additional cars in the background</li><li>✅ Demonstrated ability to detect partially visible vehicles at distance</li></ul><p><strong>Aya Vision's Response</strong>:</p><ul><li>❌ Incorrectly counted only three cars</li><li>❌ Failed to identify the black SUV directly behind the Lamborghini</li><li>❌ Misidentified vehicle types (called the white Lamborghini a \"sports car\" and mentioned a \"white SUV\" that wasn't present)</li><li>❌ Missed multiple vehicles visible in the background</li><li>❌ Created a fictional scenario with \"a car driving on the road\" (all cars were parked)</li></ul><p><strong>Analysis</strong>: This test revealed Aya Vision's limitations in accurate visual counting and identification, even with clearly visible objects. GPT-4V demonstrated superior ability to detect, count, and correctly identify vehicles throughout the scene.</p><h3 id=\"6-multilingual-capabilities-non-english-text-recognition\">6. Multilingual Capabilities: Non-English Text Recognition</h3><p><strong>The Challenge</strong>: Both models were presented with images containing non-English text (Hindi and Chinese) and asked to translate the content.</p><p><strong>Prompt</strong>: Translate the text in the attached image to English</p><p><strong>Image used:</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://hub.researchgraph.org/content/images/2025/05/Screenshot-2025-04-27-at-6.51.19-pm.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"418\" height=\"80\"></figure><p><strong>GPT-4V's Response</strong>:</p><ul><li>✅ Successfully recognized Hindi text (\"नमस्ते, आप कैसे हैं\")</li><li>✅ Correctly translated it to English (\"Hello, how are you?\")</li><li>✅ Also demonstrated ability to recognise and translate Chinese characters</li></ul><p><strong>Aya Vision's Response</strong>:</p><ul><li>❌ Failed to recognise Hindi text</li><li>❌ Unable to provide translation, and instead gave a gibberish result</li><li>❌ Similarly failed with Chinese character recognition</li><li>❌ Demonstrated significant limitation in multilingual OCR capabilities</li></ul><p><strong>Analysis</strong>: This test highlighted a critical limitation in Aya Vision's ability to process non-English text in images, while GPT-4V demonstrated strong multilingual OCR capabilities, successfully translating both Hindi and Chinese characters.</p>\n<!--kg-card-begin: html-->\n<table class=\"has-fixed-layout\"><tbody><tr><td>Prompt Type</td><td>Prompt Description</td><td>Aya Vision 32B Response</td><td>ChatGPT Response</td><td>Analysis</td></tr><tr><td>Code Analysis</td><td>Analyze a Flutter code snippet for a floating header UI</td><td>Incorrectly identified as TypeScript, missed Flutter specifics</td><td>Correctly identified as Flutter, detailed analysis, suggested fixes</td><td>ChatGPT was accurate and insightful; Aya Vision failed to recognize Flutter.</td></tr><tr><td>Image Identification</td><td>Guess the dish in the image (steamed buns)</td><td>Misidentified as char siu bao (barbecued pork buns)</td><td>Correctly identified as mushroom buns, added context</td><td>ChatGPT was accurate; Aya Vision misinterpreted the dish.</td></tr><tr><td>Visual Question Answering</td><td>How many cars are in this image?</td><td>Counted 3 cars, missed the SUV behind the Lamborghini</td><td>Counted 6 cars, including background vehicles</td><td>ChatGPT was more accurate; Aya Vision undercounted.</td></tr><tr><td>Text Summarization</td><td>Summarize a career episode text</td><td>Misinterpreted context, missed key details</td><td>Accurate summary, captured key points</td><td>ChatGPT was precise; Aya Vision was vague and incorrect.</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Multilingual Translation</td><td>Translate Hindi text in an image to English</td><td>Failed to translate Hindi and Chinese text from images</td><td>Correctly translated Hindi to \"Hello, how are you?\"</td><td>ChatGPT succeeded; Aya Vision failed despite its multilingual focus.</td></tr></tbody></table>\n<!--kg-card-end: html-->\n<h2 id=\"conclusion\">Conclusion</h2><p>While Cohere's Aya Vision model demonstrates competence in basic image recognition tasks, it currently lags behind GPT-4V in technical accuracy, hallucination control, visual reasoning capabilities, and multilingual text recognition. The significant hallucinations observed in technical contexts and counting tasks, combined with limited multilingual support, raise concerns about its reliability for professional applications requiring precision.</p><p>For users considering which vision model to implement, these findings suggest that GPT-4V currently offers more reliable performance across diverse use cases, particularly those requiring technical understanding, multilingual support, or faithful representation of image content.</p><p>As the field of multimodal AI continues to evolve rapidly, it will be interesting to see how Cohere refines Aya Vision in future iterations to address these challenges.</p>","image":"https://hub.researchgraph.org/content/images/2025/05/aya-vision-cover-image.png","state":"findable"},{"id":"https://doi.org/10.59350/cp1ww-04s35","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0007-1385-3343","type":"Person","contributorRoles":["Author"],"givenName":"Yao","familyName":"Chen"}],"date":{"published":"2025-04-27T16:00:00","updated":"2025-05-15T11:28:49"},"descriptions":[{"description":"A Deep Dive into Meta’s Latest AI Breakthrough","type":"Abstract"}],"files":[{"mimeType":"text/markdown","url":"https://api.rogue-scholar.org/posts/10.59350/cp1ww-04s35.md"},{"mimeType":"application/pdf","url":"https://api.rogue-scholar.org/posts/10.59350/cp1ww-04s35.pdf"},{"mimeType":"application/epub+zip","url":"https://api.rogue-scholar.org/posts/10.59350/cp1ww-04s35.epub"},{"mimeType":"application/xml","url":"https://api.rogue-scholar.org/posts/10.59350/cp1ww-04s35.xml"}],"identifiers":[{"identifier":"6d7dfa0e-b433-470d-aec3-e9a760487f6c","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/cp1ww-04s35","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"provider":"Crossref","publisher":{"name":"Front Matter"},"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Artificial Intelligence"},{"subject":"Toc"}],"titles":[{"title":"What is Llama 4?"}],"url":"https://hub.researchgraph.org/what-is-llama-4/","content":"<h3 id=\"introduction\">Introduction</h3><p>In April 2025, Meta AI released Llama 4, the latest iteration of its open-weight large language models (LLM) family. Building on the success of its predecessors, Llama 4 introduced groundbreaking features like native multimodal capabilities, an innovative Mixture of Experts (MoE) architecture, and an unprecedented context window of up to 10 million tokens. This blog explores what Llama 4 is, its key features, how it compares to other models, and why it matters for developers, businesses, and researchers.</p><h3 id=\"what-is-llama-4\">What is Llama 4?</h3><p>Llama 4 is a suite of large language models developed by Meta AI, designed to push the boundaries of open-source AI. Released on April 6, 2025, it includes three main variants: Llama 4 Scout, Llama 4 Maverick, and Llama 4 Behemoth (still in training). Unlike earlier Llama models, which were primarily text-based, Llama 4 is natively multimodal, meaning it can process text, images, and potentially other data types like video, making it a versatile tool for a wide range of applications.</p><p>The Llama 4 models are “open-weight,” meaning their model weights are available for download (outside the EU due to regulatory restrictions) under licenses that allow research and some commercial use. This accessibility distinguishes Llama 4 from closed-source models like GPT-4o or Claude 3.7 Sonnet, offering developers and organisations the ability to fine-tune and deploy AI on their own infrastructure.</p><h3 id=\"llama-4-version-scout-maverick-and-behemoth\">Llama 4 Version : Scout, Maverick, and Behemoth</h3><ul><li><strong>Llama 4 Scout</strong>: A lightweight, efficient model with 17 billion active parameters, optimised for single-GPU deployment. It’s ideal for tasks like multi-document summarisation, long-form reasoning, and edge applications. Its 10-million-token context window is a standout feature.</li><li><strong>Llama 4 Maverick</strong>: A general-purpose model with 400 billion total parameters and 128 experts, designed for sophisticated applications like multilingual chatbots, image understanding, and enterprise assistants. It is more resource-intensive but offers high-quality outputs.</li><li><strong>Llama 4 Behemoth</strong>: Still in training, this model is Meta’s most powerful, with 2 trillion total parameters. It’s used as a “teacher” model to distil knowledge into Scout and Maverick and is expected to rival top-tier models like Claude 3.7 Sonnet and GPT-4.5 upon release.</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/04/1ab4b435ad454f32bff068509ab183f4-1024x990.jpg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"990\"><figcaption><span style=\"white-space: pre-wrap;\">Comparison of different version of llama4(created by author)</span></figcaption></figure><h3 id=\"key-features-of-llama-4\">Key Features of Llama 4</h3><h4 id=\"1-mixture-of-experts-moe-architecture\"><strong>1. Mixture of Experts (MoE) Architecture</strong></h4><p>Llama 4 adopts a Mixture of Experts (MoE) design, a significant departure from the dense architectures of previous Llama models. In an MoE model, only a subset of parameters (called “experts”) is activated for each task, improving efficiency during training and inference. For example:</p><ul><li><strong>Llama 4 Scout</strong>: 17 billion active parameters, 16 experts, 109 billion total parameters.</li><li><strong>Llama 4 Maverick</strong>: 17 billion active parameters, 128 experts, 400 billion total parameters.</li><li><strong>Llama 4 Behemoth</strong>: 288 billion active parameters, 16 experts, 2 trillion total parameters.</li></ul><p>This architecture allows Llama 4 to deliver high performance while using fewer computational resources, making it more accessible for deployment on standard hardware like a single NVIDIA H100 GPU for Scout.</p><h4 id=\"2-native-multimodal-capabilities\">2. Native Multimodal Capabilities</h4><p>Unlike Llama 3, which had limited or no multimodal support, Llama 4 is trained from the ground up to handle text and images (and potentially video) using an early-fusion backbone. This enables tasks like:</p><ul><li><strong>Visual Question Answering</strong>: Answering questions about images, such as interpreting charts or describing scenes.</li><li><strong>Image Captioning</strong>: Generating descriptions for images.</li><li><strong>Context-Aware Generation</strong>: Combining text and visual inputs for richer outputs.</li></ul><p>For instance, Llama 4 Maverick excels in image reasoning, scoring 73.4 on the MMMU benchmark and 90.0 on Chart QA, making it competitive with models like GPT-4o.</p><h4 id=\"3-massive-context-window\">3. Massive Context Window</h4><p>Llama 4 Scout boasts a 10-million-token context window, the largest of any publicly released model, equivalent to roughly 7–8 million words. Maverick supports a 1-million-token window, still surpassing many competitors. This allows Llama 4 to:</p><ul><li>Summarise multi-thousand-page documents.</li><li>Analyse entire codebases for debugging or reasoning.</li><li>Maintain context over extended conversations or user activity histories.</li></ul><p>However, developers have noted challenges in utilising the full 10-million-token window due to memory constraints, with some third-party providers limiting context to 128,000–328,000 tokens.</p><h3 id=\"simple-question-tests\">Simple Question&nbsp;<strong>Tests</strong></h3><p>I prepared some questions for testing, and found that llama4 did not perform well on some basic questions.</p><p>First one is a simple question,&nbsp;<strong>“how many 'r's are there in the word strawberry”</strong>, we can see that llama4 gives the wrong answer but GPT-4o gives the correct answer.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-12.22.41%E2%80%AFam-1024x529.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"529\"><figcaption><span style=\"white-space: pre-wrap;\">wrong answer given by llama4</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-12.55.57%E2%80%AFam-1024x199.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"199\"><figcaption><span style=\"white-space: pre-wrap;\">correct answer given by gpt-4o</span></figcaption></figure><p>The second one is also a simple question:&nbsp;\"<strong>Of 9.12 and 9.9, which is bigger?\"</strong>, We can see that llama4 still gives the wrong answer, but GPT-4o gives the correct answer.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-1.17.31%E2%80%AFam-1024x341.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"341\"><figcaption><span style=\"white-space: pre-wrap;\">wrong answer given by llama4</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-1.26.39%E2%80%AFam-1024x276.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"276\"><figcaption><span style=\"white-space: pre-wrap;\">correct answer given by gpt-4o</span></figcaption></figure><p>The third question is a slightly more complex question:<strong>,</strong>&nbsp;<strong>“Drop a steel ball into a red wine glass, then turn the glass upside down on the table, then pick up the glass and fill it with water, then put the glass in the refrigerator for 10 minutes. Where is the steel ball now?“</strong>. This time,<strong>,</strong> both llama4 and GPT-4o gave the correct answer.</p><figure class=\"kg-card kg-image-card\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-1.51.35%E2%80%AFam-1024x412.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"412\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-1.51.16%E2%80%AFam-1024x426.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"426\"></figure><p>Next one is a math problem&nbsp;<strong>“You can use any symbols, but you cannot change the position of the numbers. How do you make this equation true? 6 5 4 1 24”</strong>, llama4 seems to have failed to solve this question and started saying nonsense, while gpt-4o solved the question correctly.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-2.12.21%E2%80%AFam-1004x1024.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1004\" height=\"1024\"><figcaption><span style=\"white-space: pre-wrap;\">Answer Given by llama4</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-1.43.23%E2%80%AFam-1024x177.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"177\"><figcaption><span style=\"white-space: pre-wrap;\">Answer given by GPT-4o</span></figcaption></figure><h3 id=\"practical-task-tests\">Practical Task Tests</h3><h4 id=\"task1-planning-task\">Task1. Planning task</h4><p>The first task is a Planning task. Here is the prompt.</p><pre><code>A takes 12 yuan to the supermarket to buy drinks. Drinks are divided into large bottles and small bottles.\n\nAmong them, the large bottle (500 ml) is 3 yuan, and the small bottle (100 ml) is 1 yuan.\n\nAfter drinking, the empty bottles can be exchanged for drinks. 3 large empty bottles can be exchanged for 1 large bottle of drink,\n\n1 large empty bottle can be exchanged for 1 small bottle of drink, 4 small empty bottles can be exchanged for 1 small bottle of drink, and 5 small empty bottles can be exchanged for 1 large bottle of drink.\n\nHow many millilitres of drink can A drink at most?\n</code></pre><p>The result is GPT-4o solves the problem correctly, and llama4 failed to solve it.<br></p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/5-1024x622.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"622\"><figcaption><span style=\"white-space: pre-wrap;\">Correct answer by gpt-4o</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"http://researchgraph.org/wp-content/uploads/2025/04/4-584x1024.jpeg\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"584\" height=\"1024\"><figcaption><span style=\"white-space: pre-wrap;\">Wrong answer by llama4</span></figcaption></figure><h4 id=\"task2-solar-system-simulation\">Task2.  Solar system simulation</h4><p>Here is the prompt for this task</p><pre><code>Create a **single HTML file** containing CSS and JavaScript to generate a **solar system simulation**.\n\nThe simulation should visualize the **eight planets** orbiting the **sun**, with different orbital paths and speeds.\nThe animation should include:\n\n- **Sun**: a glowing, pulsating sphere at the center.\n- **Eight planets**: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune,\neach with an appropriate size, distance, and orbital speed.\n- **Satellites**: some planets (such as Earth's Moon, Jupiter's Galilean moons, and Saturn's Titan) should have their own orbital motion.\n- **Asteroid belt**: a **randomly moving group of asteroids** between Mars and Jupiter.\n- **Background stars**: add a background of stars to enhance the cosmic atmosphere.\n\n**Feature requirements:**\n- The animation should use **CSS and JavaScript** to achieve **smooth orbital motion**.\n- Provide **zoom and drag** controls for better viewing.\n- Allow users to **turn orbit display on/off** for better visualization.\n- Provide an optional **info panel** that displays the name and related information of a planet when clicked.\n\nPlease provide all HTML, CSS, and JavaScript code in a **single HTML file** to make the simulation visually appealing and as consistent with scientifically plausible proportions and motion as possible.\n\n</code></pre><p>Here is the result given by gpt :</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250428142957_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250428142957.mp4\" poster=\"https://img.spacergif.org/v1/3008x1620/0a/spacer.png\" width=\"3008\" height=\"1620\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250428142957_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:27</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>Here is the result given by llama4:</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250428143648_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250428143648.mp4\" poster=\"https://img.spacergif.org/v1/3004x1448/0a/spacer.png\" width=\"3004\" height=\"1448\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250428143648_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:16</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>We can find that with only one prompt, the GPT‘s result is undoubtedly more satisfactory and meets the requirements.</p><h4 id=\"task3-implement-a-simple-and-cute-tetris-game-in-an-html-file\">Task3. Implement a simple and cute Tetris game in an HTML file</h4><p>Here is the performance of GPT-4o, It successfully implemented a playable game with the basic elements of the Tetris game.</p><figure class=\"kg-card kg-video-card kg-width-regular\" data-kg-thumbnail=\"https://researchgraph.ghost.io/content/media/2025/05/longshot20250428152208_thumb.jpg\" data-kg-custom-thumbnail=\"\">\n            <div class=\"kg-video-container\">\n                <video src=\"https://hub.researchgraph.org/content/media/2025/05/longshot20250428152208.mp4\" poster=\"https://img.spacergif.org/v1/2212x1672/0a/spacer.png\" width=\"2212\" height=\"1672\" loop=\"\" autoplay=\"\" muted=\"\" playsinline=\"\" preload=\"metadata\" style=\"background: transparent url('https://hub.researchgraph.org/content/media/2025/05/longshot20250428152208_thumb.jpg') 50% 50% / cover no-repeat;\"></video>\n                <div class=\"kg-video-overlay\">\n                    <button class=\"kg-video-large-play-icon\" aria-label=\"Play video\">\n                        <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                            <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                        </svg>\n                    </button>\n                </div>\n                <div class=\"kg-video-player-container kg-video-hide\">\n                    <div class=\"kg-video-player\">\n                        <button class=\"kg-video-play-icon\" aria-label=\"Play video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M23.14 10.608 2.253.164A1.559 1.559 0 0 0 0 1.557v20.887a1.558 1.558 0 0 0 2.253 1.392L23.14 13.393a1.557 1.557 0 0 0 0-2.785Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-pause-icon kg-video-hide\" aria-label=\"Pause video\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <rect x=\"3\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                                <rect x=\"14\" y=\"1\" width=\"7\" height=\"22\" rx=\"1.5\" ry=\"1.5\"></rect>\n                            </svg>\n                        </button>\n                        <span class=\"kg-video-current-time\">0:00</span>\n                        <div class=\"kg-video-time\">\n                            /<span class=\"kg-video-duration\">0:36</span>\n                        </div>\n                        <input type=\"range\" class=\"kg-video-seek-slider\" max=\"100\" value=\"0\">\n                        <button class=\"kg-video-playback-rate\" aria-label=\"Adjust playback speed\">1×</button>\n                        <button class=\"kg-video-unmute-icon\" aria-label=\"Unmute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M15.189 2.021a9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h1.794a.249.249 0 0 1 .221.133 9.73 9.73 0 0 0 7.924 4.85h.06a1 1 0 0 0 1-1V3.02a1 1 0 0 0-1.06-.998Z\"></path>\n                            </svg>\n                        </button>\n                        <button class=\"kg-video-mute-icon kg-video-hide\" aria-label=\"Mute\">\n                            <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n                                <path d=\"M16.177 4.3a.248.248 0 0 0 .073-.176v-1.1a1 1 0 0 0-1.061-1 9.728 9.728 0 0 0-7.924 4.85.249.249 0 0 1-.221.133H5.25a3 3 0 0 0-3 3v2a3 3 0 0 0 3 3h.114a.251.251 0 0 0 .177-.073ZM23.707 1.706A1 1 0 0 0 22.293.292l-22 22a1 1 0 0 0 0 1.414l.009.009a1 1 0 0 0 1.405-.009l6.63-6.631A.251.251 0 0 1 8.515 17a.245.245 0 0 1 .177.075 10.081 10.081 0 0 0 6.5 2.92 1 1 0 0 0 1.061-1V9.266a.247.247 0 0 1 .073-.176Z\"></path>\n                            </svg>\n                        </button>\n                        <input type=\"range\" class=\"kg-video-volume-slider\" max=\"100\" value=\"100\">\n                    </div>\n                </div>\n            </div>\n            \n        </figure><p>But the llama4 failed to generate a playable game.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/04/Screenshot-2025-04-28-at-3.23.55%E2%80%AFpm-639x1024.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"639\" height=\"1024\"></figure><h3 id=\"conclusion\">Conclusion</h3><p>After testing and real-world usage, Llama 4 falls short of the hype surrounding its release, with inconsistent performance in tasks like coding and reasoning compared to competitors like GPT-4o. Despite its innovative MoE architecture, multimodal capabilities, and massive context window, these limitations highlight the gap between benchmark scores and practical utility.</p><p>However, Llama 4’s open-weight model and cost efficiency still make it a valuable tool for developers and researchers. With Meta actively refining the model and the upcoming release of Llama 4 Behemoth, future iterations hold promise for addressing these shortcomings and delivering on the potential of open-source AI. Stay tuned for updates, and explore Llama 4 to see how it fits your AI projects!</p>","image":"https://hub.researchgraph.org/content/images/2025/05/1743996188998-1024x576-1.jpeg","state":"findable"},{"id":"","type":"BlogPost","container":{"type":"Blog","title":"Research Graph","identifier":"https://rogue-scholar.org/blogs/researchgraph","identifierType":"URL","platform":"Ghost"},"contributors":[{"id":"https://orcid.org/0009-0003-5278-0491","type":"Person","contributorRoles":["Author"],"givenName":"Yanran","familyName":"Luo"}],"date":{"published":"2025-04-27T09:01:00","updated":"2025-06-23T04:48:42"},"descriptions":[{"description":"Introduction The integration of artificial intelligence into healthcare represents one of the most transformative technological shifts in modern medicine. As AI applications proliferate across medical specialities, healthcare practitioners face an increasingly complex landscape of tools, technologies, and approaches.","type":"Abstract"}],"identifiers":[{"identifier":"62d5fd18-bc8b-4d7a-b9a0-747f9b220dca","identifierType":"UUID"},{"identifier":"https://doi.org/10.59350/sdam4-h0m63","identifierType":"GUID"}],"language":"en","license":{"id":"CC-BY-4.0","url":"https://creativecommons.org/licenses/by/4.0/legalcode"},"publisher":{"name":"Front Matter"},"subjects":[{"subject":"FOS: Computer and information sciences"},{"subject":"Ai In Health"},{"subject":"Toc"}],"titles":[{"title":"From Manual Analysis to Automated Insight: Building a Healthcare AI Taxonomy Pipeline"}],"url":"https://hub.researchgraph.org/from-manual-analysis-to-automated-insight-building-a-healthcare-ai-taxonomy-pipeline/","content":"<h2 id=\"introduction\">Introduction</h2><p>The integration of artificial intelligence into healthcare represents one of the most transformative technological shifts in modern medicine. As AI applications proliferate across medical specialities, healthcare practitioners face an increasingly complex landscape of tools, technologies, and approaches. Understanding this landscape—what applications exist, how they work, and their potential impact on clinical practice—has become crucial for medical professionals seeking to leverage AI effectively.</p><p>Traditional approaches to mapping this terrain often rely on manual literature reviews, expert opinion, and theoretical frameworks. While valuable, these methods struggle to keep pace with the rapid evolution of AI in healthcare. They are time-intensive, difficult to update, and often reflect the biases and limitations of their creators.</p><p>This article presents an alternative approach: a systematic methodology for developing and validating a practitioner-focused taxonomy of AI applications in healthcare. We begin by presenting a comprehensive taxonomy organised by medical specialities, developed through AI’s deep research of the medical literature. We then outline a vision for automating this taxonomy development process, creating a pipeline that can systematically generate, validate, and visualise taxonomies not only for healthcare AI but potentially for any domain of interest.</p><p>Using our healthcare AI taxonomy as a case study, we demonstrate how this automated approach could transform knowledge organisation, making it more systematic, evidence-based, and adaptable to rapid technological change. By bridging the gap between manual analysis and automated insight, we aim to provide a framework that helps practitioners navigate complex technological landscapes more effectively.</p><h2 id=\"the-healthcare-ai-taxonomy-a-case-study\">The Healthcare AI Taxonomy: A Case Study</h2><h3 id=\"a-practitioner-focused-framework\">A Practitioner-Focused Framework</h3><p>Understanding the landscape of AI in healthcare presents a significant challenge for practitioners. The rapid proliferation of AI applications across medical domains creates a complex environment that’s difficult to navigate without a structured framework. To address this need, we developed a comprehensive taxonomy that organises AI applications by medical specialty—aligning with how healthcare professionals typically identify and seek information.</p><p>This taxonomy comprises 16 medical specialities with significant AI integration, including Cardiology, Radiology, Ophthalmology, Pathology, Surgery, Genomics, Dermatology, and others. Within each speciality, we identified specific clinical applications where AI is making a meaningful impact, along with the technologies employed and representative research examples.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://github.com/yanranluo2/yanranluo2/blob/main/AI%20in%20Health.png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"3624\" height=\"1934\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 2. Taxonomy of “</span><b><strong style=\"white-space: pre-wrap;\">AI in Health</strong></b><span style=\"white-space: pre-wrap;\">” research and practice</span></figcaption></figure><h3 id=\"taxonomy-structure-and-insights\"><strong>Taxonomy Structure and Insights</strong></h3><p>The primary organisation by medical speciality creates an intuitive entry point for practitioners seeking to understand AI applications relevant to their field. For example, a cardiologist can quickly locate applications such as arrhythmia detection, cardiac imaging analysis, cardiovascular risk prediction, ECG interpretation, and periprocedural planning.</p><p>Our research revealed several key insights about the current state of AI in healthcare:</p><ol><li><strong>Varied adoption across specialities</strong>: Fields rich in imaging data (radiology, pathology, ophthalmology) show more advanced AI integration, while other areas are in earlier adoption stages.</li><li><strong>Common cross-cutting applications</strong>: Several application types appear across multiple specialities, including image analysis, risk prediction, workflow optimisation, diagnostic assistance, and personalised treatment planning.</li><li><strong>Evolution of AI roles</strong>: Applications are evolving from purely assistive tools to more sophisticated systems that augment clinical decision-making and, in some cases, automate specific tasks.</li><li><strong>Increasing clinical implementation</strong>: There is a gradual shift from theoretical research to real-world clinical applications, with growing regulatory approval and integration into clinical workflows.</li></ol><h3 id=\"development-methodology\"><strong>Development Methodology</strong></h3><p>This taxonomy was created through a systematic, multi-step process:</p><ul><li><strong>Deep research</strong>: Analysing the medical literature to identify AI applications across specialities using Deep Research feature</li><li><strong>Initial categorisation</strong>: Organising applications into a two-level hierarchy by speciality</li></ul><pre><code>Prompt:\n\"Search PubMed and create a comprehensive two-level taxonomy of AI applications in healthcare. \nThe first level should be organized by medical specialties (e.g., Cardiology, Radiology, etc.).\nThe second level should identify specific AI applications within each specialty.\nFor each second-level category, provide:\n1. The type of AI technology typically used\n2. A brief description of how it's applied in clinical practice\n3. One example of a representative research paper\nFocus on applications that would be most relevant to practicing clinicians.\"\n</code></pre><ul><li><strong>Validation</strong>: Confirming applications through publication metrics and clinical implementation evidence</li></ul><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/05/image-5-1024x680.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"680\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 3. Number of publications of application of AI in medical specialties</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/05/image-6.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"989\" height=\"590\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 4. Number of publications of AI applications under Cardiology subcategories</span></figcaption></figure><ul><li><strong>Refinement</strong>: Iteratively improving the taxonomy structure and content</li><li><strong>Visualisation</strong>categorisation: Creating visual representations(infographics) of the taxonomy</li></ul><p>While comprehensive and evidence-based, this manual approach required significant time and expertise. It also faces challenges in keeping pace with the rapidly evolving field of AI in healthcare. This limitation prompted us to explore how the taxonomy development process itself might be automated.</p><h3 id=\"limitations-of-manual-taxonomy-development\"><strong>Limitations of Manual Taxonomy Development</strong></h3><p>The manual process of creating our healthcare AI taxonomy, while thorough, revealed several inherent limitations:</p><ol><li><strong>Updating challenges</strong>: As new AI applications emerge daily, keeping the taxonomy current would require constant manual monitoring and updates.</li><li><strong>Scalability constraints</strong>: Expanding to additional domains or creating similar taxonomies for other fields would multiply the required effort linearly.</li><li><strong>Consistency issues</strong>: Manual categorisation decisions can vary based on the researcher’s judgment, creating potential inconsistencies.</li></ol><p>Recognising these challenges, we envisioned an automated pipeline that could transform the taxonomy development process. This approach would leverage the strengths of AI-powered research tools and data analytics to create a more efficient, consistent, and updatable taxonomy generation system.</p><h2 id=\"the-opportunity-for-automation-a-six-step-process\">The Opportunity for Automation: A Six-Step Process</h2><p>To translate our vision into a practical implementation, we designed a six-step pipeline that systematically transforms unstructured knowledge into organised taxonomies. This pipeline combines AI-powered research, data-driven validation, and automated visualisation to create a comprehensive taxonomy development system.</p><p>Each step in the pipeline addresses specific aspects of the taxonomy creation process:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://github.com/yanranluo2/yanranluo2/blob/main/Workflow%20Diagram%20Planning%20Whiteboard%20in%20Purple%20Blue%20Modern%20Professional%20Style%20(1).png?raw=true\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"4293\" height=\"1406\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 5. Design for the automation pipeline for taxonomy generation</span></figcaption></figure><h3 id=\"step-1-initial-query-and-data-collection\">Step 1: Initial Query and Data Collection</h3><ul><li><strong>Input</strong>: Domain or field of interest (e.g., “AI in Healthcare”)</li><li><strong>Process</strong>: Generate structured prompts for AI systems to collect relevant information</li><li><strong>Output</strong>: Initial search results and topic clustering</li></ul><h3 id=\"step-2-taxonomy-structure-generation\">Step 2: Taxonomy Structure Generation</h3><ul><li><strong>Input</strong>: Results from the deep research</li><li><strong>Process</strong>: AI analysis to identify hierarchical relationships</li><li><strong>Output</strong>: JSON schema with proposed taxonomy structure</li></ul><h3 id=\"step-3-literature-validation\">Step 3: Literature Validation</h3><ul><li><strong>Input</strong>: Taxonomy structure JSON</li><li><strong>Process</strong>: Automated searches of publication databases (e.g., PubMed) for each category</li><li><strong>Output</strong>: Validation data including publication counts and growth trends</li></ul><h3 id=\"step-4-taxonomy-refinement\">Step 4: Taxonomy Refinement</h3><ul><li><strong>Input</strong>: Initial taxonomy and validation data</li><li><strong>Process</strong>: AI analysis to identify gaps, overlaps, and refine categories</li><li><strong>Output</strong>: Refined taxonomy JSON structure</li></ul><h3 id=\"step-5-visualisation-generation\">Step 5: Visualisation Generation</h3><ul><li><strong>Input</strong>: Refined taxonomy JSON</li><li><strong>Process</strong>: Automated generation of visual representations</li><li><strong>Output</strong>: Visual formats such as mindmaps, infographics, and interactive diagrams</li></ul><h3 id=\"step-6-documentation-production\">Step 6: Documentation Production</h3><ul><li><strong>Input</strong>: Taxonomy JSON and visualisation files</li><li><strong>Process</strong>: AI-generated summary reports</li><li><strong>Output</strong>: Comprehensive documentation of the taxonomy</li></ul><h3 id=\"technical-implementation\"><strong>Technical Implementation</strong></h3><p>We have begun implementation of this pipeline, focusing initially on Steps 1-3 to demonstrate the core concept. Our current approach uses a combination of AI language models and publication database APIs.</p><p>For the crucial initial data collection step, we developed a two-step process that collects real publication data and uses AI to synthesise it into a structured taxonomy:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/05/image-7-1024x535.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"535\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 6. Code for extracting publications from PubMed</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/05/image-8-1024x662.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"662\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 7. Code for feeding extracted publications to LLM for taxonomy generation</span></figcaption></figure><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://researchgraph.org/wp-content/uploads/2025/05/image-9-1024x485.png\" class=\"kg-image\" alt=\"\" loading=\"lazy\" width=\"1024\" height=\"485\"><figcaption><span style=\"white-space: pre-wrap;\">Figure 8. Output from code using AI language models and publication database APIs</span></figcaption></figure><h2 id=\"current-limitations-and-future-enhancements\">Current Limitations and Future Enhancements</h2><p>While our proof-of-concept implementation demonstrates the feasibility of automated taxonomy generation, several technical challenges remain to be addressed:</p><h3 id=\"1-deep-research-api-access\">1. Deep Research API Access</h3><p>The most significant limitation is access to sophisticated deep research capabilities through APIs. Current AI models like Gemini and GPT provide limited or no direct access to their web search or deep research features through their APIs. This restricts our ability to fully automate the initial data collection step.</p><p>Our current workaround—collecting papers from PubMed and then using AI to analyse them—demonstrates the concept but doesn’t fully automate the pipeline. As AI providers expand their API capabilities, we anticipate this limitation will be resolved.</p><h3 id=\"2-structured-output-consistency\">2. Structured Output Consistency</h3><p>Ensuring consistent, well-structured output from AI models remains challenging. Our initial implementation sometimes produces output that requires additional processing to convert to a clean JSON format.</p><p>By implementing more robust post-processing of AI outputs and exploring techniques to guide models toward more consistent, structured responses.</p><h3 id=\"3-workflow-orchestration\">3.&nbsp;<strong>Workflow Orchestration</strong></h3><p>The full implementation would benefit from proper workflow orchestration using tools like Apache Airflow, n8n, to manage the pipeline execution.</p><h2 id=\"beyond-healthcare-adapting-the-pipeline-for-different-domains\">Beyond Healthcare: Adapting the Pipeline for Different Domains</h2><p>The taxonomy automation pipeline we’ve developed is inherently flexible and can be adapted to various domains beyond healthcare. The core steps—data collection, structure generation, validation, refinement, visualisation, and documentation—remain consistent, while domain-specific elements can be customised.</p><p>To adapt the pipeline to a new domain:</p><ol><li><strong>Modify data sources</strong>: Replace PubMed with domain-relevant databases (e.g., arXiv for physics, IEEE for engineering)</li><li><strong>Adjust validation metrics</strong>: Define domain-appropriate metrics for category validation</li><li><strong>Customise visualisation</strong>: Adapt visual representations to domain conventions</li><li><strong>Refine prompts</strong>: Create domain-specific prompts that capture the unique aspects of the field.</li></ol><h2 id=\"conclusion\">Conclusion</h2><p>Our exploration has taken us from a manually developed taxonomy of AI applications in healthcare to the vision and initial implementation of an automated taxonomy generation pipeline. This journey reflects broader trends in knowledge management—using AI tools themselves to better understand and organise information about rapidly evolving fields.</p><p>The healthcare AI taxonomy we developed provides a valuable framework for practitioners seeking to navigate the complex landscape of AI applications in medicine. By organising applications by speciality and providing information about technologies, clinical implementations, and research examples, it creates an accessible entry point for clinicians interested in understanding how AI might impact their practice.</p><p>Our current implementation represents the first iteration of this vision—a proof of concept that demonstrates the feasibility of using AI to analyse publication data and generate structured taxonomies. While still limited compared to our manually created taxonomy, it shows promising capabilities in extracting meaningful categories, identifying technologies, and referencing relevant literature.</p><p>The future development of this pipeline holds exciting possibilities. As AI capabilities continue to advance—particularly in terms of API access to deep research features and larger context windows—we anticipate that automated taxonomy generation will become increasingly sophisticated, comprehensive, and accurate.</p><div class=\"kg-card kg-cta-card kg-cta-bg-grey kg-cta-minimal    \" data-layout=\"minimal\">\n            \n            <div class=\"kg-cta-content\">\n                \n                \n                    <div class=\"kg-cta-content-inner\">\n                    \n                        <div class=\"kg-cta-text\">\n                            <p dir=\"ltr\"><span style=\"white-space: pre-wrap;\">Catch the latest version of this article over on Medium.com. Hit the button below to join our readers there.</span></p>\n                        </div>\n                    \n                    \n                        <a href=\"https://medium.com/@researchgraph/from-manual-analysis-to-automated-insight-building-a-healthcare-ai-taxonomy-pipeline-3e2a852dfefa?ref=hub.researchgraph.org\" class=\"kg-cta-button \" style=\"background-color: #000000; color: #ffffff;\">\n                            Learn more on Medium\n                        </a>\n                        \n                    </div>\n                \n            </div>\n        </div>","image":"https://hub.researchgraph.org/content/images/2025/05/0_-2LMML5YkZEsrRsN-1.jpg","state":"findable"}]}
