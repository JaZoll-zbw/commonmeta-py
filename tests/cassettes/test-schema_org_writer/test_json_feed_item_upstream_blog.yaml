interactions:
- request:
    body: ''
    headers:
      accept:
      - '*/*'
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      host:
      - api.rogue-scholar.org
      user-agent:
      - python-httpx/0.26.0
    method: GET
    uri: https://api.rogue-scholar.org/posts/5d14ffac-b9ac-4e20-bdc0-d9248df4e80d
  response:
    content: '{"abstract":null,"archive_url":"https://wayback.archive-it.org/22124/20231105103706/https://upstream.force11.org/attempts-at-automating-journal-subject-classification","authors":[{"name":"Esha
      Datta","url":"https://orcid.org/0000-0001-9165-2757"}],"blog":{"api":true,"archive_prefix":"https://wayback.archive-it.org/22124/20231105103706/","authors":null,"backlog":0,"canonical_url":null,"category":"humanities","created_at":1673568000,"current_feed_url":"https://upstream.force11.org/atom/","description":"The
      community blog for all things Open Research.","favicon":"https://upstream.force11.org/favicon.png","feed_format":"application/atom+xml","feed_url":"https://upstream.force11.org/atom-complete/","filter":null,"funding":null,"generator":"Ghost","generator_raw":"Ghost
      5.25","home_page_url":"https://upstream.force11.org","id":"e3952730-ffb7-4ef9-b4a5-6433d86b2819","indexed":false,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"https://scicomm.xyz/@upstream","plan":"Team","prefix":"10.54900","relative_url":null,"ror":null,"secure":true,"slug":"upstream","status":"active","title":"Upstream","updated_at":1705865031,"use_api":true,"use_mastodon":false,"user_id":"08014cf6-3335-4588-96f4-c77ac1e535b2","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Upstream","blog_slug":"upstream","content_text":"Traditionally,
      journal subject classification was done manually at\nvarying levels of granularity,
      depending on the use case for the\ninstitution. Subject classification is done
      to help collate resources by\nsubject enabling the user to discover publications
      based on different\nlevels of subject specificity. It can also be used to help
      determine\nwhere to publish and the direction a particular author may be pursuing\nin
      their research if one wants to track where their work is being\npublished. Currently,
      most subject classification is done manually as it\nis a speciality that requires
      a lot of training. However, this effort\ncan be siloed by institution or can
      be hampered by various\ninter-institutional agreements that prevent other resources
      from being\nclassified. It could also prevent a standardized approach to classifying\nitems
      if different publications in separate institutions use different\ntaxonomies
      and classification systems. Automating classification work\nsurfaces questions
      about the relevance of the taxonomy used, the\npotential bias that might exist,
      and the texts being classified.\nCurrently, journals are classified using various
      taxonomies and are\nsiloed in many systems, such as library databases or software
      for\npublishers. Providing a service that can automatically classify a text\n(and
      provide a measure of accuracy!) outside of a specific system can\ndemocratize
      access to this information across all systems. Crossref\ninfrastructure enables
      a range of services for the research community;\nwe have a wealth of metadata
      created by a very large global community.\nWe wondered how we could contribute
      in this area.\n\nIn our own metadata corpus, we had subject classifications
      for a subset\nof our journals provided by Elsevier. However, this meant that
      we were\nproviding subject information unevenly across our metadata. We wondered\nif
      we could extrapolate the information and provide the data across all\nour metadata.\n\nWe
      looked specifically at journal-level classification instead of\narticle-level
      classification for a few reasons. We had the training data\nfor journal-level
      subject classification; it was a good place to begin\nunderstanding what would
      be needed. Our work so far provides a\nfoundation for further article-level
      classification - if Crossref\ndecides to investigate further.\n\nTo start with,
      I used Elsevier''s All Science Journal Classification\nCodes\n([ASJC](https://service.elsevier.com/app/answers/detail/a_id/15181/supporthub/scopus/)),\nwhich
      have been applied to their\n[database](https://www.elsevier.com/solutions/scopus/how-scopus-works/content)\nof
      publications, which includes journals and books. We used ASJC because\nit contained
      metadata that could be parsed programmatically. If the\nproject progressed well,
      we felt that we could look at other\nclassification systems.\n\nAfter pre-processing,
      three methods (tf-idf, Embeddings, LLM) were used,\nand their performances were
      benchmarked. The following outlines the\nsteps taken for the pre-processing,
      cleaning, and implementation details\nof the methods used to predict the subject
      classification of journals.\n\n### Pre-processing of data\n\nThe Excel document
      was processed as a CSV file and has various\ninformation, including journal
      titles, the corresponding print and e-\nISSNs, and their ASJC codes. The journals
      were mostly in English but\nwere also in many other languages, such as Russian,
      Italian, Spanish,\nChinese, and others. First, there was a process to see which
      journals in\nthe Elsevier list also existed in the Crossref corpus. As of June
      2022,\nthere were 26,000 journals covered by the Elsevier database. The\njournals
      could contain one or many subject categories. For example, the\n*Journal of
      Children''s Services* has several subjects assigned to them,\nsuch as Law, Sociology
      and Political Science, Education, and Health. The\njournal titles have some
      data, but not a lot. They averaged about four\nwords per title, so more data
      was needed. First, 10 - 20 journal article\ntitles per journal were added if
      there were that many journal articles\navailable. At Crossref, a few journal
      articles contain abstracts, but\nnot all. So, for the moment, journal titles
      and their corresponding\narticle titles were the additional data points that
      were used.\n\n##### **Cleaning the data**\n\nThe data was cleaned up to remove
      stop words, various types of formulae,\nand XML from the titles. Stop words
      generally consist of articles,\npronouns, conjunctions, and other frequently
      used words. The [stop words\nlist](https://github.com/stopwords-iso/stopwords-iso)
      of all languages\nin the ISO-639 standard was used to process the titles. Some\ndomain-specific
      terms to the stop words, such as \"journal\", \"archive\",\n\"book\", \"studies\",
      and so on, were also added to the list. Formulae and\nXML tags were removed
      with regular expressions. Rare subject categories\nthat were assigned to very
      few journals (less than 50 out of 26000\njournals) \u00a0were also removed.
      The cleaned data was now ready for\nprocessing. It was split into training,
      validation, and test sets.\n\n### Methods\n\nThis particular type of classification
      is known as a multi-label\nclassification problem since zero, or many subjects
      can be assigned to a\njournal. Three methods were used to see which performed
      best.\n\n#### **TF-IDF + Linear Support Vector Classification** {#tf-idf-linear-support-vector-classification}\n\nThe
      first approach used the tf-idf and multilabel binarizer libraries\nfrom [scikit
      learn](https://scikit-learn.org/stable/index.html).\n[Tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
      is a numerical\nstatistic that is intended to reflect how important a word is
      to a\ndocument in a collection. Using tf-idf, a \u00a0number of different\nstrategies
      that can be used within a multi-label classification problem\nwere benchmarked.
      The tf-idf vectorizer and multilabel binarizer are\nPython libraries that convert
      data into machine parseable vectors.\nEssentially, the data is a table of journal
      and article titles and their\ncorresponding subjects.\n\nA baseline prediction
      was needed to benchmark the performance of the\nstrategies used. This prediction
      was made by comparing the presence of\nthe subject codes assigned to the journal
      with the most common subject\ncodes present in the corpus. The measure used
      to compare the\nperformances was the micro [F1](https://en.wikipedia.org/wiki/F-score)\nscore.
      The micro F1 score of the baseline prediction was 0.067. It shows\nthat applying
      a naive approach will provide a prediction at 6.67%\naccuracy. That measure
      provided a good starting point to get an idea of\nthe performance of subsequent
      methods.\n\nAmong the strategies used, the best-performing strategy was One
      vs Rest\nusing LinearSVC. The micro F1 score was 0.43 after processing 20,000\nfeatures
      using the validation dataset. This was a decent increase from\nthe baseline;
      however, it is still not very serviceable. In order to\nimprove performance,
      it was decided to reduce the granularity of\nsubjects. For example, the journal,
      *Journal of Children''s Services,*\nhas several subjects assigned to them, such
      as Law, Sociology and\nPolitical Science\\'', Education, and Health. Elsevier''s
      ASJC subjects are\nin hierarchies. There are several subgroups of fields within
      some\noverarching fields. For example, the group, Medicine, has several\nspecialities
      of medicine listed under it. The subjects, Social Sciences\nand Psychology work
      similarly. They are two separate fields of study,\nand the journal has articles
      that apply to either or both fields of\nstudy. The subjects listed in the \u00a0*Journal
      of Children''s Services* are\nin two different groups: Social Sciences and Psychology.
      Downgrading the\ngranularity makes the learning process a little simpler. So,
      instead of\nthe \u00a0*Journal of Children''s Services* belonging to several
      different\nsubjects, the journal now belonged to two subjects. Using the same\nstrategy,
      one vs rest with LinearSVC, we get an F1 score of 0.72 for the\nsame number
      of titles. This was a marked improvement from before. There\nwere other avenues
      that could be looked at, such as bringing in more\ndata in the form of references,
      but there were also other methods to\nlook at. We were curious about the role
      of embeddings and decided to\npursue that approach.\n\n#### **Embeddings + Linear
      Support Vector Classification** {#embeddings-linear-support-vector-classification}\n\nThis
      approach is slightly different from the tf-idf approach. For the\ntitles, we
      decided to use a model that was already trained on a\nscientific corpus. For
      this, AllenAI''s\n[SciBERT](https://github.com/allenai/scibert) was used, a
      fine-tuned\n[BERT](https://arxiv.org/abs/1810.04805) model trained on papers
      from\nthe corpus of [semanticscholar.org](https://semanticscholar.org); a tool\nprovided
      by AllenAI. The model provides an embedding: a vector\nrepresentation of the
      titles, based on the data it has already been\ntrained on. This allows it to
      provide more semantic weight on the data\nrather than simple occurrence of the
      words in the document (this occurs\nwith the previous method, tf-idf). The generation
      of the embedding took\nover 18 hours on a laptop, but after that, generating
      predictions became\nquite fast. The amount of data needed to generate this vector
      is also\nlower than the tf-idf generation. The subjects were processed similarly\nto
      before and generated a vector using the multilabel binarizer. With\n512 features
      from the titles (instead of 20,000) in the previous\napproach, the same strategy
      was used as earlier. Using the one vs rest\nstrategy with LinearSVC the strategy
      was run against the validation set\nand got a F1 score of 0.71.\n\nSo far, the
      tally is:\n\n| Method                                   | F1 Score |\n|------------------------------------------|----------|\n|
      Tf-idf + multilabel binarizer            | 0.73     |\n| SciBERT embedding +
      multilabel binarizer | 0.71     |\n\nAt this point, we were going to look into
      gathering more data points\nsuch as references and run a comparison between
      these two methods.\nHowever, large language models, especially ChatGPT, came
      into the\nzeitgeist, a few weeks into mulling over other options.\n\n#### **OpenAI:
      LLM + sentence completion** {#openai-llm-sentence-completion}\n\nOut of curiosity,
      the author looked to see what chatGPT could do.\nChatGPT was asked to figure
      out what topics an existing journal title\nbelonged to, and it came very close
      to predicting the correct answer.\nThe author also asked it to figure out to
      which topic multiple Dutch\njournal article titles belonged, and it predicted
      the correct answer\nagain. The author decided to investigate this avenue knowing
      that if\nthere were good results, open large language models would be used to
      see\nif there would be comparable results. The screenshot below shows the\nexamples
      listed above.\n\n<figure class=\"kg-card kg-image-card\">\n<img\nsrc=\"https://upstream.force11.org/content/images/2023/08/openai_experiment.png\"\nclass=\"kg-image\"
      loading=\"lazy\"\nsrcset=\"https://upstream.force11.org/content/images/size/w600/2023/08/openai_experiment.png
      600w, https://upstream.force11.org/content/images/size/w1000/2023/08/openai_experiment.png
      1000w, https://upstream.force11.org/content/images/2023/08/openai_experiment.png
      1600w\"\nsizes=\"(min-width: 720px) 720px\" width=\"1600\" height=\"1495\" />\n</figure>\n\nSubjects
      had to be processed a little differently for this model. The\nASJC codes have
      subjects in text form as well as numerical values. For\nexample, if there is
      a journal classified as \"Medicine\", it has a code\nof \"27\". The author fine-tuned
      the openAI model using their \"ada\" model\n\u00a0 (it is the fastest and the
      cheapest) and sent it some sentence\ncompletion prompts. Essentially, this means
      that the model is being\nfine-tuned into telling it what subject codes it needs
      to complete the\nsentences that it is being sent. So, suppose several different
      titles\nare sent to the model and asked to complete it with several delimited\nsubject
      codes. In that case, the model should be able to predict which\nsubject codes
      should complete the sentences. A set of prompts were\ncreated with the journal
      titles and their corresponding subject codes as\nthe sentence completion prompt
      to train the model. It looked like this:\n\n**`{\"prompt\":\"Lower Middle Ordovician
      carbon and oxygen\u2026..,\"completion\":\" 11\\n19\"}`**\n\nThe above snippet
      has several different titles where the subjects\nassigned to these titles are
      11 and 19, which are *Agricultural and\nBiological Sciences* and *Earth and
      Planetary Sciences,* respectively.\n\nThe openAI''s API was used to fine-tune
      and train a model using the above\nprompts, and \\$10.00 later, generated a
      model.\n\n<figure class=\"kg-card kg-image-card\">\n<img\nsrc=\"https://upstream.force11.org/content/images/2023/08/data-src-image-60e0df22-f6e0-4c81-adf0-fe21d2839897.png\"\nclass=\"kg-image\"
      loading=\"lazy\"\nsrcset=\"https://upstream.force11.org/content/images/size/w600/2023/08/data-src-image-60e0df22-f6e0-4c81-adf0-fe21d2839897.png
      600w, https://upstream.force11.org/content/images/size/w1000/2023/08/data-src-image-60e0df22-f6e0-4c81-adf0-fe21d2839897.png
      1000w, https://upstream.force11.org/content/images/2023/08/data-src-image-60e0df22-f6e0-4c81-adf0-fe21d2839897.png
      1600w\"\nsizes=\"(min-width: 720px) 720px\" width=\"1600\" height=\"702\" />\n</figure>\n\nThe
      validation dataset was run against the model and got a micro F1\nscore of 0.69.
      So, the tally now is:\n\n| Method                                   | F1 Score
      |\n|------------------------------------------|----------|\n| Tf-idf + multilabel
      binarizer            | 0.73     |\n| SciBERT embedding + multilabel binarizer
      | 0.71     |\n| ChatGPT + sentence completion            | 0.69     |\n\n###
      Summary\n\nSo, sad trombone, using three different methods, the F1 score is
      similar\nacross all three methods. Essentially, we needed more data for more\naccurate
      predictions. Crossref has abstracts for a subset of the\ndeposited publication
      metadata. Therefore, this data could not be used\nat this time for comparison.
      However, having that data could possibly\nyield better results. The only way
      to do that is to use a similar method\nto get those results. We do not have
      that currently, and so, for now,\n\u00a0it becomes a chicken and egg thought
      exercise. Getting even more data,\nsuch as full-text, could also produce interesting
      results, but we do not\nhave the data for that either. For now, Crossref decided
      to remove the\nexisting subject classifications that were present in some of
      our\nmetadata. We could revisit the problem later - if we have more data.\nThere
      are certainly interesting applications of these methods. We could:\n\n1.  Look
      into topic clustering across our metadata and see what\n    surfaces. This could
      also have applications in looking at the\n    research zeitgeist across various
      time periods.\n2.  Measure the similarities of embeddings with each other to
      look at\n    article similarities, which could yield interesting results in\n    recommendations
      and search.  \n\nAutomated subject classification also raises questions about
      fairness\nand bias in its algorithms and training and validation data. It would\nalso
      be productive to clearly understand how the algorithm reaches its\nconclusions.
      Therefore, any automated system must be thoroughly tested,\nand anyone using
      it should have a very good understanding of what is\nhappening within the algorithm.\n\nThis
      was an interesting exercise for the author to get acquainted with\nmachine learning
      and become familiar with some of the available\ntechniques.\n","doi":"https://doi.org/10.54900/n6dnt-xpq48","guid":"646b67ec54fc56091f756304","id":"5d14ffac-b9ac-4e20-bdc0-d9248df4e80d","image":"https://upstream.force11.org/content/images/2023/05/esha-subject-blog.jpg","indexed_at":1705662327,"language":"en","published_at":1684834305,"reference":[],"relationships":[],"summary":"Traditionally,
      journal subject classification was done manually at varying levels of granularity,
      depending on the use case for the institution. Subject classification is done
      to help collate resources by subject enabling the user to discover publications
      based on different levels of subject specificity.\n","tags":["Original Research"],"title":"Attempts
      at automating journal subject classification","updated_at":1705661262,"url":"https://upstream.force11.org/attempts-at-automating-journal-subject-classification"}

      '
    headers:
      content-encoding:
      - gzip
      content-type:
      - application/json
      date:
      - Sat, 10 Feb 2024 10:01:18 GMT
      fly-request-id:
      - 01HP97DP0K2G9S137BTHRQZD4R-fra
      ratelimit-limit:
      - '15'
      ratelimit-remaining:
      - '3'
      ratelimit-reset:
      - '44'
      server:
      - Fly/ba9e227a (2024-01-26)
      transfer-encoding:
      - chunked
      vary:
      - Origin
      via:
      - 1.1 fly.io
    http_version: HTTP/1.1
    status_code: 200
version: 1
